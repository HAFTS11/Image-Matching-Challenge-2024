{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8fcfce0d",
   "metadata": {
    "papermill": {
     "duration": 0.025005,
     "end_time": "2024-05-21T18:01:10.281573",
     "exception": false,
     "start_time": "2024-05-21T18:01:10.256568",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "# üåü Image Matching Challenge 2024 - Hexathlon üåü\n",
    "\n",
    "## Overview\n",
    "Welcome, brave adventurers, to the thrilling Image Matching Challenge 2024 - Hexathlon! Here, amidst the vast expanse of digital landscapes, your mission, should you choose to accept it, is to reconstruct 3D scenes from 2D images across six distinct domains. From ancient ruins to bustling city streets, from serene forests to the twinkling night sky, each domain presents its own set of challenges to overcome.\n",
    "\n",
    "Last year's Image Matching Challenge was but a prelude to this grand spectacle. This year, stakes are raised, weaving together an intricate tapestry of diverse scenarios into a single competition. Let's  test our mettle and push the boundaries of computer vision\n",
    "\n",
    "## üõ†Ô∏è Setup Environment\n",
    "Prepare yourselves, intrepid explorers, for the journey ahead requires meticulous preparation. Fear not, for we shall guide you through the arcane rituals of environment setup with clarity and wit:\n",
    "\n",
    "1. **Install Necessary Libraries**: üìö\n",
    "   To equip your arsenal with the tools needed for this quest, execute the following commands:\n",
    "\n",
    "   ```bash\n",
    "   !pip install -r /kaggle/input/check-image-orientation/requirements.txt\n",
    "   !pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps\n",
    "   ```\n",
    "\n",
    "2. **Setup Cache and Checkpoints**: üîí\n",
    "   Fortify your cache and checkpoints with the resilience of ancient guardians:\n",
    "\n",
    "   ```bash\n",
    "   !mkdir -p /root/.cache/torch/hub/checkpoints\n",
    "   !cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/\n",
    "   !cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/\n",
    "   !cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth\n",
    "   !cp /kaggle/input/check-image-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/\n",
    "   ```\n",
    "\n",
    "3. **Import Essential Libraries**: üì¶\n",
    "   Arm yourselves with the knowledge and power of the ancients with these sacred incantations:\n",
    "\n",
    "   ```python\n",
    "   import libraries\n",
    "   from pathlib import Path\n",
    "   from copy import deepcopy\n",
    "   import numpy as np\n",
    "   import math\n",
    "   import pandas as pd\n",
    "   import pandas.api.types\n",
    "   from itertools import combinations\n",
    "   import sys, torch, h5py, pycolmap, datetime\n",
    "   from PIL import Image\n",
    "   from pathlib import Path\n",
    "   import torch.nn.functional as F\n",
    "   import torchvision.transforms.functional as TF\n",
    "   import kornia as K\n",
    "   import kornia.feature as KF\n",
    "   from lightglue.utils import load_image\n",
    "   from lightglue import LightGlue, ALIKED, match_pair\n",
    "   from transformers import AutoImageProcessor, AutoModel\n",
    "   from check_orientation.pre_trained_models import create_model\n",
    "   sys.path.append(\"/kaggle/input/colmap-db-import\")\n",
    "   import sqlite3\n",
    "   import os, argparse, h5py, warnings\n",
    "   import numpy as np\n",
    "   from tqdm import tqdm\n",
    "   from PIL import Image, ExifTags\n",
    "   from database import COLMAPDatabase, image_ids_to_pair_id\n",
    "   from h5_to_db import *\n",
    "   ```\n",
    "\n",
    "Now, adventurers, with your environment fortified and your libraries in hand, you stand poised at the precipice of discovery, ready to delve into the depths of image matching and registration!\n",
    "\n",
    "## üß© Concepts Explored\n",
    "Behold, noble souls, the sacred knowledge that shall guide you on your quest:\n",
    "\n",
    "1. **Feature Matching**: üåü\n",
    "   Traverse the realm of feature matching, where keypoints align and images harmonize through the magic of computer vision.\n",
    "\n",
    "2. **RANSAC (Random Sample Consensus)**: üé≤\n",
    "   Embrace the randomness of RANSAC, a robust method that triumphs over outliers and guides you on the path to accurate image registration.\n",
    "\n",
    "3. **Sparse Reconstruction**: üåå\n",
    "   Witness the reconstruction of 3D scenes from sparse image correspondences, a feat achieved through the enigmatic algorithms of sparse reconstruction.\n",
    "\n",
    "4. **Mean Average Accuracy (mAA)**: üéØ\n",
    "   Gauge the accuracy of your endeavors with the noble metric of mAA, measuring the alignment of images with a precision fit for champions.\n",
    "\n",
    "5. **Homogeneous Transformation Matrix**: üîÑ\n",
    "   Let the homogeneous transformation matrix be your guide through the labyrinth of rigid transformations, as you align images and estimate camera poses with finesse.\n",
    "\n",
    "6. **Affine Transformation**: üñåÔ∏è\n",
    "   Marvel at the versatility of affine transformations, shaping images with the strokes of a digital brush to correct distortions and align perspectives.\n",
    "\n",
    "7. **Quaternion Representation**: üîÆ\n",
    "   Peer into the depths of quaternion representation, where rotations in 3D space unfold with elegance and grace, free from the shackles of gimbal lock.\n",
    "\n",
    "## üëâ [View Notebook on Kaggle](https://www.kaggle.com/code/zulqarnainalipk/imc-24-explained/)\n",
    "\n",
    "\n",
    "\n",
    "## Acknowledgments üôè\n",
    "I acknowledge the organizers of theCzech Technical University in Prague for providing the dataset and the competition platform. Additionally, I extend my gratitude to the computer vision community for their contributions to image registration techniques and algorithms.\n",
    "\n",
    "Let's embark on the journey of image matching and registration! Feel free to reach out if you have any questions or need assistance along the way.\n",
    "üëâ [Visit my Profile](https://www.kaggle.com/zulqarnainalipk) üëà\n",
    "\n",
    "\n",
    "## üí¨ Share Your Thoughts! üí°\n",
    "\n",
    "Your feedback is like treasure to us! Your brilliant ideas and insights fuel our ongoing improvement. Got something to say, ask, or suggest? Don't hold back!\n",
    "\n",
    "üì¨ Drop me a line via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3578fe3",
   "metadata": {
    "papermill": {
     "duration": 0.023868,
     "end_time": "2024-05-21T18:01:10.330603",
     "exception": false,
     "start_time": "2024-05-21T18:01:10.306735",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a510e7",
   "metadata": {
    "papermill": {
     "duration": 0.023809,
     "end_time": "2024-05-21T18:01:10.378468",
     "exception": false,
     "start_time": "2024-05-21T18:01:10.354659",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Pip Install libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a62b9728",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:01:10.427787Z",
     "iopub.status.busy": "2024-05-21T18:01:10.427449Z",
     "iopub.status.idle": "2024-05-21T18:01:56.039025Z",
     "shell.execute_reply": "2024-05-21T18:01:56.037980Z"
    },
    "papermill": {
     "duration": 45.639309,
     "end_time": "2024-05-21T18:01:56.041573",
     "exception": false,
     "start_time": "2024-05-21T18:01:10.402264",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "get_ipython().system('pip install -r /kaggle/input/check-image-orientation/requirements.txt')\n",
    "get_ipython().system('pip install --no-index /kaggle/input/imc2024-packages-lightglue-rerun-kornia/* --no-deps')\n",
    "get_ipython().system('mkdir -p /root/.cache/torch/hub/checkpoints')\n",
    "get_ipython().system('cp /kaggle/input/aliked/pytorch/aliked-n16/1/* /root/.cache/torch/hub/checkpoints/')\n",
    "get_ipython().system('cp /kaggle/input/lightglue/pytorch/aliked/1/* /root/.cache/torch/hub/checkpoints/')\n",
    "get_ipython().system('cp /kaggle/input/lightglue/pytorch/aliked/1/aliked_lightglue.pth /root/.cache/torch/hub/checkpoints/aliked_lightglue_v0-1_arxiv-pth')\n",
    "get_ipython().system('cp /kaggle/input/check-image-orientation/2020-11-16_resnext50_32x4d.zip /root/.cache/torch/hub/checkpoints/')\n",
    "clear_output(wait=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56801812",
   "metadata": {
    "papermill": {
     "duration": 0.023877,
     "end_time": "2024-05-21T18:01:56.090173",
     "exception": false,
     "start_time": "2024-05-21T18:01:56.066296",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e619751f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:01:56.140139Z",
     "iopub.status.busy": "2024-05-21T18:01:56.139473Z",
     "iopub.status.idle": "2024-05-21T18:02:14.821197Z",
     "shell.execute_reply": "2024-05-21T18:02:14.820378Z"
    },
    "papermill": {
     "duration": 18.709468,
     "end_time": "2024-05-21T18:02:14.823526",
     "exception": false,
     "start_time": "2024-05-21T18:01:56.114058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-21 18:02:05.621843: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-05-21 18:02:05.621981: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-05-21 18:02:05.738741: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117: UserWarning: Mapping deprecated model name swsl_resnext50_32x4d to current resnext50_32x4d.fb_swsl_ig1b_ft_in1k.\n",
      "  model = create_fn(\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import math\n",
    "import pandas as pd\n",
    "import pandas.api.types\n",
    "from itertools import combinations\n",
    "import sys, torch, h5py, pycolmap, datetime\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms.functional as TF\n",
    "import kornia as K\n",
    "import kornia.feature as KF\n",
    "from lightglue.utils import load_image\n",
    "from lightglue import LightGlue, ALIKED, match_pair\n",
    "from transformers import AutoImageProcessor, AutoModel\n",
    "from check_orientation.pre_trained_models import create_model\n",
    "sys.path.append(\"/kaggle/input/colmap-db-import\")\n",
    "import sqlite3\n",
    "import os, argparse, h5py, warnings\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image, ExifTags\n",
    "from database import COLMAPDatabase, image_ids_to_pair_id\n",
    "from h5_to_db import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fd3e0b",
   "metadata": {
    "papermill": {
     "duration": 0.025433,
     "end_time": "2024-05-21T18:02:14.874323",
     "exception": false,
     "start_time": "2024-05-21T18:02:14.848890",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad18a97f",
   "metadata": {
    "papermill": {
     "duration": 0.023988,
     "end_time": "2024-05-21T18:02:14.922731",
     "exception": false,
     "start_time": "2024-05-21T18:02:14.898743",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**:\n",
    "\n",
    "\n",
    "1. **Setting the Maximum Image ID**:\n",
    "   - `2**31 - 1` calculates the largest 32-bit signed integer value, which is 2 raised to the power of 31 minus 1. This value is 2147483647.\n",
    "   - The result is stored in the variable `MAX_IMAGE_ID`.\n",
    "   ```python\n",
    "   MAX_IMAGE_ID = 2**31 - 1\n",
    "   ```\n",
    "   - **Concept**: This is often used to set a maximum value for identifiers, ensuring they fit within a 32-bit integer range. This is crucial for databases or systems where the ID needs to be a 32-bit integer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37ca1ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:14.972995Z",
     "iopub.status.busy": "2024-05-21T18:02:14.972224Z",
     "iopub.status.idle": "2024-05-21T18:02:14.976500Z",
     "shell.execute_reply": "2024-05-21T18:02:14.975684Z"
    },
    "papermill": {
     "duration": 0.031464,
     "end_time": "2024-05-21T18:02:14.978307",
     "exception": false,
     "start_time": "2024-05-21T18:02:14.946843",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "\n",
    "MAX_IMAGE_ID = 2**31 - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c8eccf",
   "metadata": {
    "papermill": {
     "duration": 0.024079,
     "end_time": "2024-05-21T18:02:15.026648",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.002569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381cfbed",
   "metadata": {
    "papermill": {
     "duration": 0.024263,
     "end_time": "2024-05-21T18:02:15.115048",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.090785",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üì∑ Creating the Cameras Table in SQL üõ†Ô∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - The provided SQL statement is used to create a table named `cameras` if it doesn't already exist. This ensures that running the script multiple times won't result in errors due to the table already existing.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS cameras (\n",
    "       camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "       model INTEGER NOT NULL,\n",
    "       width INTEGER NOT NULL,\n",
    "       height INTEGER NOT NULL,\n",
    "       params BLOB,\n",
    "       prior_focal_length INTEGER NOT NULL\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Table Columns**:\n",
    "   - **`camera_id`**: \n",
    "     - `INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL`: This defines the `camera_id` as the primary key for the table. It will automatically increment with each new entry, ensuring a unique identifier for each record.\n",
    "   - **`model`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the model number of the camera, ensuring it's an integer and cannot be null.\n",
    "   - **`width`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the width of the camera's image sensor or resolution, ensuring it's an integer and cannot be null.\n",
    "   - **`height`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the height of the camera's image sensor or resolution, ensuring it's an integer and cannot be null.\n",
    "   - **`params`**:\n",
    "     - `BLOB`: This column stores additional parameters for the camera in binary large object format. BLOB is used to store data such as images, multimedia, and other large data types.\n",
    "   - **`prior_focal_length`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the prior focal length of the camera, ensuring it's an integer and cannot be null.\n",
    "\n",
    "3. **Concepts**:\n",
    "   - **SQL Data Types**:\n",
    "     - **INTEGER**: Used to store whole numbers.\n",
    "     - **PRIMARY KEY**: A unique identifier for table records. `AUTOINCREMENT` ensures each new record gets a unique value automatically.\n",
    "     - **NOT NULL**: Ensures that a column cannot have a NULL value.\n",
    "     - **BLOB**: Stands for Binary Large Object, used to store large amounts of binary data.\n",
    "   - **SQL Table Creation**:\n",
    "     - `CREATE TABLE IF NOT EXISTS`: Ensures that the table creation command does not fail if the table already exists.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **SQL Data Types**:\n",
    "   - Understanding the different SQL data types and their uses is crucial for database design.\n",
    "   - **Source**: [SQL Data Types](https://www.w3schools.com/sql/sql_datatypes.asp)\n",
    "\n",
    "2. **SQL PRIMARY KEY and AUTOINCREMENT**:\n",
    "   - The use of primary keys and the AUTOINCREMENT attribute is essential for ensuring unique identifiers in a database.\n",
    "   - **Source**: [PRIMARY KEY and AUTOINCREMENT](https://www.sqlite.org/autoinc.html)\n",
    "\n",
    "3. **BLOB Data Type**:\n",
    "   - Learning about the BLOB data type is important for storing large binary data in a database.\n",
    "   - **Source**: [BLOB Data Type](https://www.sqlite.org/datatype3.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb16f93b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.165476Z",
     "iopub.status.busy": "2024-05-21T18:02:15.164840Z",
     "iopub.status.idle": "2024-05-21T18:02:15.169102Z",
     "shell.execute_reply": "2024-05-21T18:02:15.168302Z"
    },
    "papermill": {
     "duration": 0.031652,
     "end_time": "2024-05-21T18:02:15.170951",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.139299",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_CAMERAS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS cameras (\n",
    "    camera_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    model INTEGER NOT NULL,\n",
    "    width INTEGER NOT NULL,\n",
    "    height INTEGER NOT NULL,\n",
    "    params BLOB,\n",
    "    prior_focal_length INTEGER NOT NULL)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3d1d4b",
   "metadata": {
    "papermill": {
     "duration": 0.024143,
     "end_time": "2024-05-21T18:02:15.219600",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.195457",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f52463",
   "metadata": {
    "papermill": {
     "duration": 0.02511,
     "end_time": "2024-05-21T18:02:15.269329",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.244219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üñºÔ∏è Creating the Descriptors Table in SQL üîç\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - This SQL statement creates a table named `descriptors` if it doesn't already exist. This ensures that the script can be run multiple times without causing errors due to the table already existing.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS descriptors (\n",
    "       image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "       rows INTEGER NOT NULL,\n",
    "       cols INTEGER NOT NULL,\n",
    "       data BLOB,\n",
    "       FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Table Columns**:\n",
    "   - **`image_id`**:\n",
    "     - `INTEGER PRIMARY KEY NOT NULL`: This defines `image_id` as the primary key for the table. It must be unique and cannot be null.\n",
    "     - **Concept**: In this context, `image_id` is used to uniquely identify each record in the `descriptors` table and link it to a corresponding record in the `images` table.\n",
    "   - **`rows`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of rows in the descriptor matrix, ensuring it's an integer and cannot be null.\n",
    "   - **`cols`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of columns in the descriptor matrix, ensuring it's an integer and cannot be null.\n",
    "   - **`data`**:\n",
    "     - `BLOB`: This column stores the descriptor data in a binary large object format. BLOB is used for storing large amounts of binary data.\n",
    "   - **`FOREIGN KEY` Constraint**:\n",
    "     - `FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE`: This establishes a foreign key relationship between `image_id` in the `descriptors` table and `image_id` in the `images` table. The `ON DELETE CASCADE` clause ensures that if a record in the `images` table is deleted, the corresponding records in the `descriptors` table will also be deleted automatically.\n",
    "     - **Concept**: This ensures referential integrity between the `descriptors` and `images` tables, maintaining a consistent relationship.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **SQL Foreign Key Constraints**:\n",
    "   - Understanding foreign keys and how they enforce referential integrity is crucial for relational database design.\n",
    "   - **Source**: [SQL Foreign Key](https://www.w3schools.com/sql/sql_foreignkey.asp)\n",
    "\n",
    "2. **SQL ON DELETE CASCADE**:\n",
    "   - Learning how the `ON DELETE CASCADE` clause works helps in designing databases where dependent records are automatically managed.\n",
    "   - **Source**: [SQL ON DELETE CASCADE](https://www.sqlshack.com/sql-server-on-delete-cascade-and-on-update-cascade/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02fc200d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.327662Z",
     "iopub.status.busy": "2024-05-21T18:02:15.326831Z",
     "iopub.status.idle": "2024-05-21T18:02:15.331486Z",
     "shell.execute_reply": "2024-05-21T18:02:15.330502Z"
    },
    "papermill": {
     "duration": 0.037353,
     "end_time": "2024-05-21T18:02:15.333588",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.296235",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_DESCRIPTORS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS descriptors (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20df280e",
   "metadata": {
    "papermill": {
     "duration": 0.028818,
     "end_time": "2024-05-21T18:02:15.391305",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.362487",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81abd15",
   "metadata": {
    "papermill": {
     "duration": 0.027731,
     "end_time": "2024-05-21T18:02:15.447616",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.419885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üì∏ Creating the Images Table in SQL üñºÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - This SQL statement creates a table named `images` if it doesn't already exist, preventing errors if the table is created multiple times.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS images (\n",
    "       image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "       name TEXT NOT NULL UNIQUE,\n",
    "       camera_id INTEGER NOT NULL,\n",
    "       prior_qw REAL,\n",
    "       prior_qx REAL,\n",
    "       prior_qy REAL,\n",
    "       prior_qz REAL,\n",
    "       prior_tx REAL,\n",
    "       prior_ty REAL,\n",
    "       prior_tz REAL,\n",
    "       CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "       FOREIGN KEY(camera_id) REFERENCES cameras(camera_id)\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Dynamic Constraint**:\n",
    "   - The `{}` placeholder in the `CHECK` constraint is dynamically filled with the value of `MAX_IMAGE_ID`, ensuring that `image_id` is within the valid range of `0` to `MAX_IMAGE_ID - 1`.\n",
    "   ```python\n",
    "   .format(MAX_IMAGE_ID)\n",
    "   ```\n",
    "\n",
    "3. **Table Columns**:\n",
    "   - **`image_id`**:\n",
    "     - `INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL`: This defines `image_id` as the primary key with an auto-increment feature and ensures it cannot be null.\n",
    "   - **`name`**:\n",
    "     - `TEXT NOT NULL UNIQUE`: This column stores the image name, ensuring it's unique and not null.\n",
    "   - **`camera_id`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the ID of the camera that took the image, ensuring it's not null.\n",
    "   - **`prior_qw`, `prior_qx`, `prior_qy`, `prior_qz`**:\n",
    "     - `REAL`: These columns store the prior quaternion values for the image.\n",
    "   - **`prior_tx`, `prior_ty`, `prior_tz`**:\n",
    "     - `REAL`: These columns store the prior translation values for the image.\n",
    "   - **`CONSTRAINT image_id_check`**:\n",
    "     - `CHECK(image_id >= 0 and image_id < {})`: Ensures `image_id` is within a valid range.\n",
    "   - **`FOREIGN KEY(camera_id)`**:\n",
    "     - `REFERENCES cameras(camera_id)`: Ensures that `camera_id` in the `images` table corresponds to `camera_id` in the `cameras` table, maintaining referential integrity.\n",
    "\n",
    "4. **Concepts**:\n",
    "   - **SQL Data Types**:\n",
    "     - **INTEGER**: Used to store whole numbers.\n",
    "     - **REAL**: Used to store floating-point numbers.\n",
    "     - **TEXT**: Used to store text strings.\n",
    "     - **PRIMARY KEY**: A unique identifier for table records with `AUTOINCREMENT` to automatically generate a unique value.\n",
    "     - **NOT NULL**: Ensures that a column cannot have a NULL value.\n",
    "     - **UNIQUE**: Ensures all values in a column are unique.\n",
    "   - **FOREIGN KEY**:\n",
    "     - Ensures that values in the `camera_id` column correspond to values in the `camera_id` column of the `cameras` table.\n",
    "   - **CHECK Constraint**:\n",
    "     - Ensures that the values in the `image_id` column fall within the specified range.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **SQL CHECK Constraint**:\n",
    "   - Learning how to use the `CHECK` constraint to enforce conditions on column values is essential for data integrity.\n",
    "   - **Source**: [SQL CHECK Constraint](https://www.w3schools.com/sql/sql_check.asp)\n",
    "\n",
    "2. **SQL Data Types**:\n",
    "   - Understanding SQL data types like `INTEGER`, `REAL`, and `TEXT` is crucial for designing database schemas.\n",
    "   - **Source**: [SQL Data Types](https://www.w3schools.com/sql/sql_datatypes.asp)\n",
    "\n",
    "3. **SQL FOREIGN KEY Constraints**:\n",
    "   - Ensuring referential integrity with foreign key constraints is a fundamental aspect of relational database design.\n",
    "   - **Source**: [SQL Foreign Key](https://www.w3schools.com/sql/sql_foreignkey.asp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc24eaaa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.501284Z",
     "iopub.status.busy": "2024-05-21T18:02:15.500988Z",
     "iopub.status.idle": "2024-05-21T18:02:15.505583Z",
     "shell.execute_reply": "2024-05-21T18:02:15.504698Z"
    },
    "papermill": {
     "duration": 0.03227,
     "end_time": "2024-05-21T18:02:15.507444",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.475174",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "CREATE_IMAGES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS images (\n",
    "    image_id INTEGER PRIMARY KEY AUTOINCREMENT NOT NULL,\n",
    "    name TEXT NOT NULL UNIQUE,\n",
    "    camera_id INTEGER NOT NULL,\n",
    "    prior_qw REAL,\n",
    "    prior_qx REAL,\n",
    "    prior_qy REAL,\n",
    "    prior_qz REAL,\n",
    "    prior_tx REAL,\n",
    "    prior_ty REAL,\n",
    "    prior_tz REAL,\n",
    "    CONSTRAINT image_id_check CHECK(image_id >= 0 and image_id < {}),\n",
    "    FOREIGN KEY(camera_id) REFERENCES cameras(camera_id))\n",
    "\"\"\".format(MAX_IMAGE_ID)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d721ed",
   "metadata": {
    "papermill": {
     "duration": 0.024493,
     "end_time": "2024-05-21T18:02:15.556366",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.531873",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed6b848",
   "metadata": {
    "papermill": {
     "duration": 0.024502,
     "end_time": "2024-05-21T18:02:15.605571",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.581069",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîß Creating the Two-View Geometries Table in SQL üìê\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - This SQL statement creates a table named `two_view_geometries` if it doesn't already exist. This ensures that the script can be run multiple times without causing errors due to the table already existing.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "       pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "       rows INTEGER NOT NULL,\n",
    "       cols INTEGER NOT NULL,\n",
    "       data BLOB,\n",
    "       config INTEGER NOT NULL,\n",
    "       F BLOB,\n",
    "       E BLOB,\n",
    "       H BLOB\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Table Columns**:\n",
    "   - **`pair_id`**:\n",
    "     - `INTEGER PRIMARY KEY NOT NULL`: This defines `pair_id` as the primary key for the table, ensuring it is unique and not null.\n",
    "   - **`rows`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of rows in the data matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`cols`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of columns in the data matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`data`**:\n",
    "     - `BLOB`: This column stores the geometric data as a binary large object. BLOB is used for storing large amounts of binary data.\n",
    "   - **`config`**:\n",
    "     - `INTEGER NOT NULL`: This column stores configuration data as an integer and cannot be null.\n",
    "   - **`F`**:\n",
    "     - `BLOB`: This column stores the fundamental matrix as a binary large object.\n",
    "   - **`E`**:\n",
    "     - `BLOB`: This column stores the essential matrix as a binary large object.\n",
    "   - **`H`**:\n",
    "     - `BLOB`: This column stores the homography matrix as a binary large object.\n",
    "\n",
    "3. **Concepts**:\n",
    "   - **Fundamental Matrix (F)**:\n",
    "     - Represents the epipolar geometry between two views.\n",
    "   - **Essential Matrix (E)**:\n",
    "     - Encodes the relative rotation and translation between two cameras.\n",
    "   - **Homography Matrix (H)**:\n",
    "     - Relates the coordinates of corresponding points in two views assuming a planar scene.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Epipolar Geometry and Fundamental Matrix**:\n",
    "   - Understanding the fundamental matrix is crucial for working with epipolar geometry in computer vision.\n",
    "   - **Source**: [Epipolar Geometry](https://en.wikipedia.org/wiki/Epipolar_geometry)\n",
    "\n",
    "2. **Essential Matrix**:\n",
    "   - The essential matrix is key in recovering the relative pose between two calibrated views.\n",
    "   - **Source**: [Essential Matrix](https://en.wikipedia.org/wiki/Essential_matrix)\n",
    "\n",
    "3. **Homography Matrix**:\n",
    "   - A homography matrix is used in computer vision to perform transformations between different planes.\n",
    "   - **Source**: [Homography (Computer Vision)](https://en.wikipedia.org/wiki/Homography_(computer_vision))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ea901828",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.656140Z",
     "iopub.status.busy": "2024-05-21T18:02:15.655822Z",
     "iopub.status.idle": "2024-05-21T18:02:15.660134Z",
     "shell.execute_reply": "2024-05-21T18:02:15.659286Z"
    },
    "papermill": {
     "duration": 0.031869,
     "end_time": "2024-05-21T18:02:15.662057",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.630188",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_TWO_VIEW_GEOMETRIES_TABLE = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS two_view_geometries (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    config INTEGER NOT NULL,\n",
    "    F BLOB,\n",
    "    E BLOB,\n",
    "    H BLOB)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38016274",
   "metadata": {
    "papermill": {
     "duration": 0.0243,
     "end_time": "2024-05-21T18:02:15.711010",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.686710",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d7893e",
   "metadata": {
    "papermill": {
     "duration": 0.024515,
     "end_time": "2024-05-21T18:02:15.760025",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.735510",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîë Creating the Keypoints Table in SQL üìù\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - This SQL statement creates a table named `keypoints` if it doesn't already exist. This ensures that the script can be run multiple times without causing errors due to the table already existing.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS keypoints (\n",
    "       image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "       rows INTEGER NOT NULL,\n",
    "       cols INTEGER NOT NULL,\n",
    "       data BLOB,\n",
    "       FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Table Columns**:\n",
    "   - **`image_id`**:\n",
    "     - `INTEGER PRIMARY KEY NOT NULL`: This defines `image_id` as the primary key for the table, ensuring it is unique and not null.\n",
    "   - **`rows`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of rows in the keypoints matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`cols`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of columns in the keypoints matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`data`**:\n",
    "     - `BLOB`: This column stores the keypoints data as a binary large object. BLOB is used for storing large amounts of binary data.\n",
    "\n",
    "3. **Foreign Key Constraint**:\n",
    "   - The `FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE` constraint ensures referential integrity by linking the `image_id` column in the `keypoints` table to the `image_id` column in the `images` table. The `ON DELETE CASCADE` clause ensures that if a record in the `images` table is deleted, the corresponding records in the `keypoints` table will also be deleted automatically.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Keypoints in Computer Vision**:\n",
    "   - Understanding keypoints and their role in computer vision tasks is fundamental for feature extraction and matching.\n",
    "   - **Source**: [Feature Detection and Description - OpenCV Documentation](https://docs.opencv.org/3.4/db/d27/tutorial_py_table_of_contents_feature2d.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fe307f88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.814080Z",
     "iopub.status.busy": "2024-05-21T18:02:15.813467Z",
     "iopub.status.idle": "2024-05-21T18:02:15.817586Z",
     "shell.execute_reply": "2024-05-21T18:02:15.816767Z"
    },
    "papermill": {
     "duration": 0.033933,
     "end_time": "2024-05-21T18:02:15.819409",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.785476",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_KEYPOINTS_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS keypoints (\n",
    "    image_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB,\n",
    "    FOREIGN KEY(image_id) REFERENCES images(image_id) ON DELETE CASCADE)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43e6372",
   "metadata": {
    "papermill": {
     "duration": 0.024622,
     "end_time": "2024-05-21T18:02:15.869023",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.844401",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360944cf",
   "metadata": {
    "papermill": {
     "duration": 0.024527,
     "end_time": "2024-05-21T18:02:15.919512",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.894985",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîó Creating the Matches Table in SQL üîÑ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Table Creation Statement**:\n",
    "   - This SQL statement creates a table named `matches` if it doesn't already exist. This ensures that the script can be run multiple times without causing errors due to the table already existing.\n",
    "   ```sql\n",
    "   CREATE TABLE IF NOT EXISTS matches (\n",
    "       pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "       rows INTEGER NOT NULL,\n",
    "       cols INTEGER NOT NULL,\n",
    "       data BLOB\n",
    "   )\n",
    "   ```\n",
    "\n",
    "2. **Table Columns**:\n",
    "   - **`pair_id`**:\n",
    "     - `INTEGER PRIMARY KEY NOT NULL`: This defines `pair_id` as the primary key for the table, ensuring it is unique and not null.\n",
    "   - **`rows`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of rows in the matches matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`cols`**:\n",
    "     - `INTEGER NOT NULL`: This column stores the number of columns in the matches matrix, ensuring it is an integer and cannot be null.\n",
    "   - **`data`**:\n",
    "     - `BLOB`: This column stores the matches data as a binary large object. BLOB is used for storing large amounts of binary data.\n",
    "\n",
    "3. **Concepts**:\n",
    "   - **Matches**:\n",
    "     - Matches represent correspondences between keypoints in different images obtained through feature matching algorithms.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Feature Matching in Computer Vision**:\n",
    "   - Understanding feature matching and its applications is crucial for various computer vision tasks such as image stitching, object recognition, and 3D reconstruction.\n",
    "   - **Source**: [Feature Matching - OpenCV Documentation](https://docs.opencv.org/3.4/dc/dc3/tutorial_py_matcher.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "351bb4ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:15.970466Z",
     "iopub.status.busy": "2024-05-21T18:02:15.969702Z",
     "iopub.status.idle": "2024-05-21T18:02:15.973851Z",
     "shell.execute_reply": "2024-05-21T18:02:15.973070Z"
    },
    "papermill": {
     "duration": 0.031563,
     "end_time": "2024-05-21T18:02:15.975692",
     "exception": false,
     "start_time": "2024-05-21T18:02:15.944129",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_MATCHES_TABLE = \"\"\"CREATE TABLE IF NOT EXISTS matches (\n",
    "    pair_id INTEGER PRIMARY KEY NOT NULL,\n",
    "    rows INTEGER NOT NULL,\n",
    "    cols INTEGER NOT NULL,\n",
    "    data BLOB)\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b349084",
   "metadata": {
    "papermill": {
     "duration": 0.032002,
     "end_time": "2024-05-21T18:02:16.032529",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.000527",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a6e2c5",
   "metadata": {
    "papermill": {
     "duration": 0.024683,
     "end_time": "2024-05-21T18:02:16.082341",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.057658",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üè∑Ô∏è Creating Name Index and All Tables in SQL üõ†Ô∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Name Index Creation Statement**:\n",
    "   - This SQL statement creates a unique index named `index_name` on the `name` column of the `images` table if it doesn't already exist. This ensures that the `name` column values are unique, preventing duplicate entries for image names.\n",
    "   ```sql\n",
    "   CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\n",
    "   ```\n",
    "\n",
    "2. **Combining Table Creation Statements**:\n",
    "   - The `CREATE_ALL` statement combines all table creation statements into a single SQL command. Each table creation statement is separated by a semicolon (`;`), allowing multiple SQL commands to be executed in sequence.\n",
    "   \n",
    "   ```python\n",
    "   CREATE_ALL = \"; \".join([\n",
    "       CREATE_CAMERAS_TABLE,\n",
    "       CREATE_IMAGES_TABLE,\n",
    "       CREATE_KEYPOINTS_TABLE,\n",
    "       CREATE_DESCRIPTORS_TABLE,\n",
    "       CREATE_MATCHES_TABLE,\n",
    "       CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "       CREATE_NAME_INDEX\n",
    "   ])\n",
    "   ```\n",
    "\n",
    "3. **Concepts**:\n",
    "   - **SQL Index**:\n",
    "     - An index is a database structure that improves the speed of data retrieval operations on a database table at the cost of additional space and decreased performance on data modification operations. A unique index ensures that no two rows of a table have duplicate values in the indexed column or columns.\n",
    "   - **JOIN**:\n",
    "     - The `JOIN` function in Python is used to concatenate a sequence of strings with a specified separator.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **SQL Index**:\n",
    "   - Understanding indexes in databases and their types is crucial for optimizing query performance.\n",
    "   - **Source**: [SQL Index - Tutorialspoint](https://www.tutorialspoint.com/sql/sql-indexes.htm)\n",
    "\n",
    "2. **Python Join Method**:\n",
    "   - Learning about the `join()` method in Python is important for concatenating strings efficiently.\n",
    "   - **Source**: [Python Join Method - w3schools](https://www.w3schools.com/python/ref_string_join.asp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b1ecd7c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.133340Z",
     "iopub.status.busy": "2024-05-21T18:02:16.133001Z",
     "iopub.status.idle": "2024-05-21T18:02:16.137529Z",
     "shell.execute_reply": "2024-05-21T18:02:16.136696Z"
    },
    "papermill": {
     "duration": 0.032219,
     "end_time": "2024-05-21T18:02:16.139360",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.107141",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "CREATE_NAME_INDEX = \\\n",
    "    \"CREATE UNIQUE INDEX IF NOT EXISTS index_name ON images(name)\"\n",
    "\n",
    "CREATE_ALL = \"; \".join([\n",
    "    CREATE_CAMERAS_TABLE,\n",
    "    CREATE_IMAGES_TABLE,\n",
    "    CREATE_KEYPOINTS_TABLE,\n",
    "    CREATE_DESCRIPTORS_TABLE,\n",
    "    CREATE_MATCHES_TABLE,\n",
    "    CREATE_TWO_VIEW_GEOMETRIES_TABLE,\n",
    "    CREATE_NAME_INDEX\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e477338",
   "metadata": {
    "papermill": {
     "duration": 0.024492,
     "end_time": "2024-05-21T18:02:16.188724",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.164232",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9872241",
   "metadata": {
    "papermill": {
     "duration": 0.024738,
     "end_time": "2024-05-21T18:02:16.238529",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.213791",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "\n",
    "The `image_ids_to_pair_id` function takes two image IDs, `image_id1` and `image_id2`, and returns a pair ID computed based on these IDs. The pair ID is calculated by concatenating the two image IDs with the maximum image ID `MAX_IMAGE_ID`. \n",
    "\n",
    "Here's how the function works:\n",
    "\n",
    "- It compares the two image IDs `image_id1` and `image_id2`. If `image_id1` is greater than `image_id2`, it swaps their values. This step ensures consistency in the pair ID calculation regardless of the order of the input image IDs.\n",
    "\n",
    "- The pair ID is then calculated by multiplying the smaller image ID (`image_id1`) by the maximum image ID (`MAX_IMAGE_ID`) and adding the larger image ID (`image_id2`). This concatenation ensures uniqueness of the pair ID for any pair of image IDs.\n",
    "\n",
    "- The calculated pair ID is returned as the result.\n",
    "\n",
    "Here's the function in code:\n",
    "\n",
    "```python\n",
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    # Ensure image_id1 is smaller than or equal to image_id2\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    # Calculate pair ID\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "```\n",
    "\n",
    "### Example:\n",
    "```python\n",
    "image_id1 = 10\n",
    "image_id2 = 20\n",
    "pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "print(pair_id)  # Output: 20310\n",
    "```\n",
    "\n",
    "In this example, `image_id1` is smaller than `image_id2`, so the pair ID is calculated as `image_id1 * MAX_IMAGE_ID + image_id2`, resulting in `20310`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95b02530",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.289262Z",
     "iopub.status.busy": "2024-05-21T18:02:16.288959Z",
     "iopub.status.idle": "2024-05-21T18:02:16.293371Z",
     "shell.execute_reply": "2024-05-21T18:02:16.292636Z"
    },
    "papermill": {
     "duration": 0.031994,
     "end_time": "2024-05-21T18:02:16.295299",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.263305",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def image_ids_to_pair_id(image_id1, image_id2):\n",
    "    if image_id1 > image_id2:\n",
    "        image_id1, image_id2 = image_id2, image_id1\n",
    "    return image_id1 * MAX_IMAGE_ID + image_id2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7e9166",
   "metadata": {
    "papermill": {
     "duration": 0.024549,
     "end_time": "2024-05-21T18:02:16.344694",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.320145",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ca2e9ff",
   "metadata": {
    "papermill": {
     "duration": 0.024857,
     "end_time": "2024-05-21T18:02:16.394312",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.369455",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîÑ Converting Pair ID to Image IDs in Python üî¢\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Function Purpose**:\n",
    "   - The function `pair_id_to_image_ids` takes a `pair_id` and converts it back to the original image IDs (`image_id1` and `image_id2`). This is the reverse operation of the `image_ids_to_pair_id` function.\n",
    "\n",
    "2. **Parameter and Return**:\n",
    "   - **Parameter**:\n",
    "     - `pair_id`: An integer that uniquely identifies a pair of image IDs.\n",
    "   - **Return**:\n",
    "     - A tuple containing two integers: `image_id1` and `image_id2`.\n",
    "\n",
    "3. **Steps in the Function**:\n",
    "   - **Calculate `image_id2`**:\n",
    "     - `image_id2 = pair_id % MAX_IMAGE_ID`: This line calculates `image_id2` by taking the modulus of `pair_id` with `MAX_IMAGE_ID`. The modulus operation returns the remainder when `pair_id` is divided by `MAX_IMAGE_ID`, effectively isolating `image_id2`.\n",
    "   - **Calculate `image_id1`**:\n",
    "     - `image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID`: This line calculates `image_id1` by subtracting `image_id2` from `pair_id` and then dividing the result by `MAX_IMAGE_ID`. This isolates `image_id1`.\n",
    "   - **Return the Image IDs**:\n",
    "     - `return image_id1, image_id2`: This returns the tuple of `image_id1` and `image_id2`.\n",
    "\n",
    "4. **Concepts**:\n",
    "   - **Modulus Operator (`%`)**:\n",
    "     - The modulus operator returns the remainder of a division operation. It's useful for breaking down numbers, particularly in encoding and decoding schemes.\n",
    "   - **Integer Division (`/`)**:\n",
    "     - In Python 3, the division operator (`/`) returns a float. For integer division that truncates towards zero, you would typically use `//`, but here we use `/` since `pair_id` was constructed using multiplication, which naturally results in an integer outcome when reversed.\n",
    "\n",
    "5. **Edge Cases**:\n",
    "   - This function assumes that `pair_id` was generated using the `image_ids_to_pair_id` function and thus `image_id1` will always be less than or equal to `image_id2`.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **Modulus Operator**:\n",
    "   - Understanding the modulus operator is essential for various algorithms, particularly those involving cyclical structures and encoding/decoding.\n",
    "   - **Source**: [Modulus Operator - Real Python](https://realpython.com/python-modulo-operator/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1055893",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.445623Z",
     "iopub.status.busy": "2024-05-21T18:02:16.445322Z",
     "iopub.status.idle": "2024-05-21T18:02:16.449573Z",
     "shell.execute_reply": "2024-05-21T18:02:16.448764Z"
    },
    "papermill": {
     "duration": 0.032246,
     "end_time": "2024-05-21T18:02:16.451466",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.419220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pair_id_to_image_ids(pair_id):\n",
    "    image_id2 = pair_id % MAX_IMAGE_ID\n",
    "    image_id1 = (pair_id - image_id2) / MAX_IMAGE_ID\n",
    "    return image_id1, image_id2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcd3378",
   "metadata": {
    "papermill": {
     "duration": 0.024559,
     "end_time": "2024-05-21T18:02:16.500824",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.476265",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5960df3",
   "metadata": {
    "papermill": {
     "duration": 0.024626,
     "end_time": "2024-05-21T18:02:16.550185",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.525559",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üõ†Ô∏è Converting Array to BLOB in Python üì¶\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Function Purpose**:\n",
    "   - The function `array_to_blob` converts a NumPy array to a binary large object (BLOB). This is useful for storing arrays in a database as BLOBs.\n",
    "\n",
    "2. **Parameter and Return**:\n",
    "   - **Parameter**:\n",
    "     - `array`: A NumPy array that needs to be converted to a BLOB.\n",
    "   - **Return**:\n",
    "     - A binary representation of the NumPy array, suitable for storage as a BLOB.\n",
    "\n",
    "3. **Python Version Check**:\n",
    "   - The function uses a global variable `IS_PYTHON3` to check if the Python version is 3 or higher. This variable should be defined elsewhere in the code, typically as:\n",
    "     ```python\n",
    "     IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "     ```\n",
    "   - **Python 3**:\n",
    "     - `array.tostring()`: In Python 3, this method converts the array to a string of bytes. Note that `tostring()` is deprecated in favor of `tobytes()` in recent versions of NumPy.\n",
    "   - **Python 2**:\n",
    "     - `np.getbuffer(array)`: In Python 2, this method converts the array to a buffer object.\n",
    "\n",
    "\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **NumPy Arrays**:\n",
    "   - Understanding NumPy arrays and their methods is crucial for scientific computing and data manipulation in Python.\n",
    "   - **Source**: [NumPy Array Documentation](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n",
    "\n",
    "2. **BLOB Data Type**:\n",
    "   - The BLOB (Binary Large Object) data type is used to store large binary data such as images, multimedia files, and raw data.\n",
    "   - **Source**: [BLOB Data Type - Database Guide](https://www.tutorialspoint.com/sql/sql-blob-data-type.htm)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa3777d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.601130Z",
     "iopub.status.busy": "2024-05-21T18:02:16.600815Z",
     "iopub.status.idle": "2024-05-21T18:02:16.606172Z",
     "shell.execute_reply": "2024-05-21T18:02:16.605494Z"
    },
    "papermill": {
     "duration": 0.033044,
     "end_time": "2024-05-21T18:02:16.608022",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.574978",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def array_to_blob(array):\n",
    "    if IS_PYTHON3:\n",
    "        return array.tostring()\n",
    "    else:\n",
    "        return np.getbuffer(array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52260c29",
   "metadata": {
    "papermill": {
     "duration": 0.025312,
     "end_time": "2024-05-21T18:02:16.662175",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.636863",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab53a341",
   "metadata": {
    "papermill": {
     "duration": 0.024858,
     "end_time": "2024-05-21T18:02:16.712051",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.687193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîÑ Converting BLOB to Array in Python üîÑ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Function Purpose**:\n",
    "   - The function `blob_to_array` converts a binary large object (BLOB) back into a NumPy array. This is useful for retrieving and manipulating arrays stored as BLOBs in a database.\n",
    "\n",
    "2. **Parameters and Return**:\n",
    "   - **Parameters**:\n",
    "     - `blob`: A BLOB that needs to be converted back into a NumPy array.\n",
    "     - `dtype`: The data type of the resulting NumPy array (e.g., `np.float32`).\n",
    "     - `shape`: The shape of the resulting NumPy array, with a default value of `(-1,)`, meaning a single-dimensional array with inferred length.\n",
    "   - **Return**:\n",
    "     - A NumPy array reconstructed from the BLOB.\n",
    "\n",
    "3. **Python Version Check**:\n",
    "   - The function uses a global variable `IS_PYTHON3` to check if the Python version is 3 or higher.\n",
    "     ```python\n",
    "     IS_PYTHON3 = sys.version_info[0] >= 3\n",
    "     ```\n",
    "   - **Python 3**:\n",
    "     - `np.fromstring(blob, dtype=dtype).reshape(*shape)`: Converts the BLOB to a NumPy array of the specified data type and reshapes it. Note that `np.fromstring()` is deprecated in favor of `np.frombuffer()`, so we should update this.\n",
    "   - **Python 2**:\n",
    "     - `np.frombuffer(blob, dtype=dtype).reshape(*shape)`: Converts the BLOB to a NumPy array of the specified data type and reshapes it.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "903393e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.763421Z",
     "iopub.status.busy": "2024-05-21T18:02:16.763077Z",
     "iopub.status.idle": "2024-05-21T18:02:16.769214Z",
     "shell.execute_reply": "2024-05-21T18:02:16.767967Z"
    },
    "papermill": {
     "duration": 0.035477,
     "end_time": "2024-05-21T18:02:16.772278",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.736801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def blob_to_array(blob, dtype, shape=(-1,)):\n",
    "    if IS_PYTHON3:\n",
    "        return np.fromstring(blob, dtype=dtype).reshape(*shape)\n",
    "    else:\n",
    "        return np.frombuffer(blob, dtype=dtype).reshape(*shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb95780",
   "metadata": {
    "papermill": {
     "duration": 0.032768,
     "end_time": "2024-05-21T18:02:16.838662",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.805894",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f296cb7",
   "metadata": {
    "papermill": {
     "duration": 0.026758,
     "end_time": "2024-05-21T18:02:16.890970",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.864212",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üì∏ COLMAP Database Management with SQLite in Python üóÑÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Class Definition: `COLMAPDatabase`**\n",
    "   - The `COLMAPDatabase` class extends `sqlite3.Connection`, adding functionality specific to managing a COLMAP database.\n",
    "   - **COLMAP**: A general-purpose Structure-from-Motion (SfM) and Multi-View Stereo (MVS) pipeline for 3D reconstruction.\n",
    "\n",
    "2. **Static Method: `connect`**\n",
    "   - **Purpose**: To establish a connection to a database using the `COLMAPDatabase` class.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db = COLMAPDatabase.connect('path_to_database.db')\n",
    "     ```\n",
    "\n",
    "3. **Constructor: `__init__`**\n",
    "   - **Purpose**: To initialize the database connection and provide methods to create various tables.\n",
    "   - **Table Creation Methods**: \n",
    "     - `create_tables`, `create_cameras_table`, `create_descriptors_table`, `create_images_table`, `create_two_view_geometries_table`, `create_keypoints_table`, `create_matches_table`, `create_name_index`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.create_tables()\n",
    "     ```\n",
    "\n",
    "4. **Method: `add_camera`**\n",
    "   - **Purpose**: To insert a new camera into the `cameras` table.\n",
    "   - **Parameters**:\n",
    "     - `model`, `width`, `height`, `params`, `prior_focal_length`, `camera_id`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_camera(model, width, height, params)\n",
    "     ```\n",
    "\n",
    "5. **Method: `add_image`**\n",
    "   - **Purpose**: To insert a new image into the `images` table.\n",
    "   - **Parameters**:\n",
    "     - `name`, `camera_id`, `prior_q`, `prior_t`, `image_id`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_image(name, camera_id)\n",
    "     ```\n",
    "\n",
    "6. **Method: `add_keypoints`**\n",
    "   - **Purpose**: To insert keypoints associated with an image into the `keypoints` table.\n",
    "   - **Parameters**:\n",
    "     - `image_id`, `keypoints`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_keypoints(image_id, keypoints)\n",
    "     ```\n",
    "\n",
    "7. **Method: `add_descriptors`**\n",
    "   - **Purpose**: To insert descriptors associated with an image into the `descriptors` table.\n",
    "   - **Parameters**:\n",
    "     - `image_id`, `descriptors`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_descriptors(image_id, descriptors)\n",
    "     ```\n",
    "\n",
    "8. **Method: `add_matches`**\n",
    "   - **Purpose**: To insert matches between two images into the `matches` table.\n",
    "   - **Parameters**:\n",
    "     - `image_id1`, `image_id2`, `matches`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_matches(image_id1, image_id2, matches)\n",
    "     ```\n",
    "\n",
    "9. **Method: `add_two_view_geometry`**\n",
    "   - **Purpose**: To insert two-view geometry information between two images into the `two_view_geometries` table.\n",
    "   - **Parameters**:\n",
    "     - `image_id1`, `image_id2`, `matches`, `F`, `E`, `H`, `config`.\n",
    "   - **Usage**:\n",
    "     ```python\n",
    "     db.add_two_view_geometry(image_id1, image_id2, matches)\n",
    "     ```\n",
    "\n",
    "### Detailed Concepts\n",
    "\n",
    "1. **NumPy Arrays**:\n",
    "   - Used extensively for numerical operations and data storage.\n",
    "   - **Methods**: `np.asarray()`, `np.zeros()`, `np.float32`, `np.uint8`, `np.uint32`, `np.float64`.\n",
    "\n",
    "2. **BLOB Conversion**:\n",
    "   - Functions `array_to_blob` and `blob_to_array` handle the conversion between NumPy arrays and binary large objects for storage in the database.\n",
    "\n",
    "3. **SQLite3 Connection**:\n",
    "   - **Inheritance**: The class inherits from `sqlite3.Connection` to leverage SQLite's capabilities.\n",
    "   - **Methods**: `executescript`, `execute`.\n",
    "\n",
    "4. **Lambda Functions**:\n",
    "   - Used for concise, anonymous functions.\n",
    "   - Example: `self.create_tables = lambda: self.executescript(CREATE_ALL)`\n",
    "\n",
    "5. **SQL Commands**:\n",
    "   - **INSERT INTO**: Used for inserting new records into tables.\n",
    "   - **CREATE TABLE**: Used for creating new tables in the database.\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **NumPy Arrays**:\n",
    "   - Understanding NumPy array operations and data types.\n",
    "   - **Source**: [NumPy Array Documentation](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n",
    "\n",
    "2. **SQLite3 in Python**:\n",
    "   - Learning about SQLite3 database operations in Python.\n",
    "   - **Source**: [SQLite3 - Python Documentation](https://docs.python.org/3/library/sqlite3.html)\n",
    "\n",
    "3. **BLOB Data Type**:\n",
    "   - Understanding BLOBs for storing binary data.\n",
    "   - **Source**: [BLOB Data Type - Database Guide](https://www.tutorialspoint.com/sql/sql-blob-data-type.htm)\n",
    "\n",
    "4. **Lambda Functions in Python**:\n",
    "   - Learning about lambda functions for concise code.\n",
    "   - **Source**: [Lambda Functions - Real Python](https://realpython.com/python-lambda/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8066ef6e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:16.944023Z",
     "iopub.status.busy": "2024-05-21T18:02:16.943212Z",
     "iopub.status.idle": "2024-05-21T18:02:16.963009Z",
     "shell.execute_reply": "2024-05-21T18:02:16.962086Z"
    },
    "papermill": {
     "duration": 0.048805,
     "end_time": "2024-05-21T18:02:16.965068",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.916263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "class COLMAPDatabase(sqlite3.Connection):\n",
    "\n",
    "    @staticmethod\n",
    "    def connect(database_path):\n",
    "        return sqlite3.connect(database_path, factory=COLMAPDatabase)\n",
    "\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super(COLMAPDatabase, self).__init__(*args, **kwargs)\n",
    "\n",
    "        self.create_tables = lambda: self.executescript(CREATE_ALL)\n",
    "        self.create_cameras_table = \\\n",
    "            lambda: self.executescript(CREATE_CAMERAS_TABLE)\n",
    "        self.create_descriptors_table = \\\n",
    "            lambda: self.executescript(CREATE_DESCRIPTORS_TABLE)\n",
    "        self.create_images_table = \\\n",
    "            lambda: self.executescript(CREATE_IMAGES_TABLE)\n",
    "        self.create_two_view_geometries_table = \\\n",
    "            lambda: self.executescript(CREATE_TWO_VIEW_GEOMETRIES_TABLE)\n",
    "        self.create_keypoints_table = \\\n",
    "            lambda: self.executescript(CREATE_KEYPOINTS_TABLE)\n",
    "        self.create_matches_table = \\\n",
    "            lambda: self.executescript(CREATE_MATCHES_TABLE)\n",
    "        self.create_name_index = lambda: self.executescript(CREATE_NAME_INDEX)\n",
    "\n",
    "    def add_camera(self, model, width, height, params,\n",
    "                   prior_focal_length=False, camera_id=None):\n",
    "        params = np.asarray(params, np.float64)\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO cameras VALUES (?, ?, ?, ?, ?, ?)\",\n",
    "            (camera_id, model, width, height, array_to_blob(params),\n",
    "             prior_focal_length))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_image(self, name, camera_id,\n",
    "                  prior_q=np.zeros(4), prior_t=np.zeros(3), image_id=None):\n",
    "        cursor = self.execute(\n",
    "            \"INSERT INTO images VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (image_id, name, camera_id, prior_q[0], prior_q[1], prior_q[2],\n",
    "             prior_q[3], prior_t[0], prior_t[1], prior_t[2]))\n",
    "        return cursor.lastrowid\n",
    "\n",
    "    def add_keypoints(self, image_id, keypoints):\n",
    "        assert(len(keypoints.shape) == 2)\n",
    "        assert(keypoints.shape[1] in [2, 4, 6])\n",
    "\n",
    "        keypoints = np.asarray(keypoints, np.float32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO keypoints VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + keypoints.shape + (array_to_blob(keypoints),))\n",
    "\n",
    "    def add_descriptors(self, image_id, descriptors):\n",
    "        descriptors = np.ascontiguousarray(descriptors, np.uint8)\n",
    "        self.execute(\n",
    "            \"INSERT INTO descriptors VALUES (?, ?, ?, ?)\",\n",
    "            (image_id,) + descriptors.shape + (array_to_blob(descriptors),))\n",
    "\n",
    "    def add_matches(self, image_id1, image_id2, matches):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        self.execute(\n",
    "            \"INSERT INTO matches VALUES (?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches),))\n",
    "\n",
    "    def add_two_view_geometry(self, image_id1, image_id2, matches,\n",
    "                              F=np.eye(3), E=np.eye(3), H=np.eye(3), config=2):\n",
    "        assert(len(matches.shape) == 2)\n",
    "        assert(matches.shape[1] == 2)\n",
    "\n",
    "        if image_id1 > image_id2:\n",
    "            matches = matches[:,::-1]\n",
    "\n",
    "        pair_id = image_ids_to_pair_id(image_id1, image_id2)\n",
    "        matches = np.asarray(matches, np.uint32)\n",
    "        F = np.asarray(F, dtype=np.float64)\n",
    "        E = np.asarray(E, dtype=np.float64)\n",
    "        H = np.asarray(H, dtype=np.float64)\n",
    "        self.execute(\n",
    "            \"INSERT INTO two_view_geometries VALUES (?, ?, ?, ?, ?, ?, ?, ?)\",\n",
    "            (pair_id,) + matches.shape + (array_to_blob(matches), config,\n",
    "             array_to_blob(F), array_to_blob(E), array_to_blob(H)))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195f12a",
   "metadata": {
    "papermill": {
     "duration": 0.024917,
     "end_time": "2024-05-21T18:02:17.015031",
     "exception": false,
     "start_time": "2024-05-21T18:02:16.990114",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c297cb5e",
   "metadata": {
    "papermill": {
     "duration": 0.025018,
     "end_time": "2024-05-21T18:02:17.065284",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.040266",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìù Example Usage of COLMAP Database Management with SQLite üóÑÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **Function Definition: `example_usage`**\n",
    "   - This function demonstrates the use of the `COLMAPDatabase` class to create and manipulate a COLMAP database.\n",
    "\n",
    "2. **Imports and Argument Parsing**\n",
    "   - Import necessary modules: `os`, `argparse`, `numpy`.\n",
    "   - **Argument Parsing**: Define and parse the command-line argument `--database_path` with a default value of `\"database.db\"`.\n",
    "     ```python\n",
    "     parser = argparse.ArgumentParser()\n",
    "     parser.add_argument(\"--database_path\", default=\"database.db\")\n",
    "     args, unknown = parser.parse_known_args()\n",
    "     ```\n",
    "\n",
    "3. **Database Path Check**\n",
    "   - Check if the database path already exists to avoid overwriting an existing database.\n",
    "     ```python\n",
    "     if os.path.exists(args.database_path):\n",
    "         print(\"ERROR: database path already exists -- will not modify it.\")\n",
    "         return\n",
    "     ```\n",
    "\n",
    "4. **Database Connection and Table Creation**\n",
    "   - Establish a connection to the database and create all required tables.\n",
    "     ```python\n",
    "     db = COLMAPDatabase.connect(args.database_path)\n",
    "     db.create_tables()\n",
    "     ```\n",
    "\n",
    "5. **Adding Cameras**\n",
    "   - Create dummy camera entries and add them to the database.\n",
    "     ```python\n",
    "     model1, width1, height1, params1 = 0, 1024, 768, np.array((1024., 512., 384.))\n",
    "     model2, width2, height2, params2 = 2, 1024, 768, np.array((1024., 512., 384., 0.1))\n",
    "     camera_id1 = db.add_camera(model1, width1, height1, params1)\n",
    "     camera_id2 = db.add_camera(model2, width2, height2, params2)\n",
    "     ```\n",
    "\n",
    "6. **Adding Images**\n",
    "   - Create dummy image entries and add them to the database.\n",
    "     ```python\n",
    "     image_id1 = db.add_image(\"image1.png\", camera_id1)\n",
    "     image_id2 = db.add_image(\"image2.png\", camera_id1)\n",
    "     image_id3 = db.add_image(\"image3.png\", camera_id2)\n",
    "     image_id4 = db.add_image(\"image4.png\", camera_id2)\n",
    "     ```\n",
    "\n",
    "7. **Adding Keypoints**\n",
    "   - Create and add random keypoints associated with each image.\n",
    "     ```python\n",
    "     num_keypoints = 1000\n",
    "     keypoints1 = np.random.rand(num_keypoints, 2) * (width1, height1)\n",
    "     keypoints2 = np.random.rand(num_keypoints, 2) * (width1, height1)\n",
    "     keypoints3 = np.random.rand(num_keypoints, 2) * (width2, height2)\n",
    "     keypoints4 = np.random.rand(num_keypoints, 2) * (width2, height2)\n",
    "     db.add_keypoints(image_id1, keypoints1)\n",
    "     db.add_keypoints(image_id2, keypoints2)\n",
    "     db.add_keypoints(image_id3, keypoints3)\n",
    "     db.add_keypoints(image_id4, keypoints4)\n",
    "     ```\n",
    "\n",
    "8. **Adding Matches**\n",
    "   - Create and add random matches between pairs of images.\n",
    "     ```python\n",
    "     M = 50\n",
    "     matches12 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "     matches23 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "     matches34 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "     db.add_matches(image_id1, image_id2, matches12)\n",
    "     db.add_matches(image_id2, image_id3, matches23)\n",
    "     db.add_matches(image_id3, image_id4, matches34)\n",
    "     ```\n",
    "\n",
    "9. **Commit and Verification**\n",
    "   - Commit changes to the database.\n",
    "   - Verify the stored cameras, keypoints, and matches.\n",
    "     ```python\n",
    "     db.commit()\n",
    "     # Verify cameras\n",
    "     rows = db.execute(\"SELECT * FROM cameras\")\n",
    "     camera_id, model, width, height, params, prior = next(rows)\n",
    "     params = blob_to_array(params, np.float64)\n",
    "     assert camera_id == camera_id1\n",
    "     assert model == model1 and width == width1 and height == height1\n",
    "     assert np.allclose(params, params1)\n",
    "     # Verify keypoints\n",
    "     keypoints = dict(\n",
    "         (image_id, blob_to_array(data, np.float32, (-1, 2)))\n",
    "         for image_id, data in db.execute(\n",
    "             \"SELECT image_id, data FROM keypoints\"))\n",
    "     assert np.allclose(keypoints[image_id1], keypoints1)\n",
    "     assert np.allclose(keypoints[image_id2], keypoints2)\n",
    "     assert np.allclose(keypoints[image_id3], keypoints3)\n",
    "     assert np.allclose(keypoints[image_id4], keypoints4)\n",
    "     # Verify matches\n",
    "     pair_ids = [image_ids_to_pair_id(*pair) for pair in\n",
    "                 ((image_id1, image_id2),\n",
    "                  (image_id2, image_id3),\n",
    "                  (image_id3, image_id4))]\n",
    "     matches = dict(\n",
    "         (pair_id_to_image_ids(pair_id),\n",
    "          blob_to_array(data, np.uint32, (-1, 2)))\n",
    "         for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\")\n",
    "     )\n",
    "     assert np.all(matches[(image_id1, image_id2)] == matches12)\n",
    "     assert np.all(matches[(image_id2, image_id3)] == matches23)\n",
    "     assert np.all(matches[(image_id3, image_id4)] == matches34)\n",
    "     ```\n",
    "\n",
    "10. **Clean Up**\n",
    "    - Close the database connection and remove the database file to clean up.\n",
    "      ```python\n",
    "      db.close()\n",
    "      if os.path.exists(args.database_path):\n",
    "          os.remove(args.database_path)\n",
    "      ```\n",
    "\n",
    "\n",
    "### Study Sources\n",
    "\n",
    "1. **NumPy Arrays**:\n",
    "   - Understanding NumPy array operations and data types.\n",
    "   - **Source**: [NumPy Array Documentation](https://numpy.org/doc/stable/reference/generated/numpy.array.html)\n",
    "\n",
    "2. **SQLite3 in Python**:\n",
    "   - Learning about SQLite3 database operations in Python.\n",
    "   - **Source**: [SQLite3 - Python Documentation](https://docs.python.org/3/library/sqlite3.html)\n",
    "\n",
    "3. **BLOB Data Type**:\n",
    "   - Understanding BLOBs for storing binary data.\n",
    "   - **Source**: [BLOB Data Type - Database Guide](https://www.tutorialspoint.com/sql/sql-blob-data-type.htm)\n",
    "\n",
    "4. **Lambda Functions in Python**:\n",
    "   - Learning about lambda functions for concise code.\n",
    "   - **Source**: [Lambda Functions - Real Python](https://realpython.com/python-lambda/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2556341",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:17.117432Z",
     "iopub.status.busy": "2024-05-21T18:02:17.117119Z",
     "iopub.status.idle": "2024-05-21T18:02:17.226656Z",
     "shell.execute_reply": "2024-05-21T18:02:17.225735Z"
    },
    "papermill": {
     "duration": 0.138305,
     "end_time": "2024-05-21T18:02:17.228848",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.090543",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_24/2422509778.py:3: DeprecationWarning: tostring() is deprecated. Use tobytes() instead.\n",
      "  return array.tostring()\n",
      "/tmp/ipykernel_24/2589672960.py:3: DeprecationWarning: The binary mode of fromstring is deprecated, as it behaves surprisingly on unicode inputs. Use frombuffer instead\n",
      "  return np.fromstring(blob, dtype=dtype).reshape(*shape)\n"
     ]
    }
   ],
   "source": [
    "def example_usage():\n",
    "    import os\n",
    "    import argparse\n",
    "    import numpy as np\n",
    "    #from your_database_module import COLMAPDatabase, blob_to_array, image_ids_to_pair_id, pair_id_to_image_ids  # replace with actual import paths\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--database_path\", default=\"database.db\")\n",
    "    args, unknown = parser.parse_known_args()\n",
    "\n",
    "    if os.path.exists(args.database_path):\n",
    "        print(\"ERROR: database path already exists -- will not modify it.\")\n",
    "        return\n",
    "\n",
    "    # Open the database.\n",
    "    db = COLMAPDatabase.connect(args.database_path)\n",
    "\n",
    "    # For convenience, try creating all the tables upfront.\n",
    "    db.create_tables()\n",
    "\n",
    "    # Create dummy cameras.\n",
    "    model1, width1, height1, params1 = 0, 1024, 768, np.array((1024., 512., 384.))\n",
    "    model2, width2, height2, params2 = 2, 1024, 768, np.array((1024., 512., 384., 0.1))\n",
    "\n",
    "    camera_id1 = db.add_camera(model1, width1, height1, params1)\n",
    "    camera_id2 = db.add_camera(model2, width2, height2, params2)\n",
    "\n",
    "    # Create dummy images.\n",
    "    image_id1 = db.add_image(\"image1.png\", camera_id1)\n",
    "    image_id2 = db.add_image(\"image2.png\", camera_id1)\n",
    "    image_id3 = db.add_image(\"image3.png\", camera_id2)\n",
    "    image_id4 = db.add_image(\"image4.png\", camera_id2)\n",
    "\n",
    "    # Create dummy keypoints.\n",
    "    num_keypoints = 1000\n",
    "    keypoints1 = np.random.rand(num_keypoints, 2) * (width1, height1)\n",
    "    keypoints2 = np.random.rand(num_keypoints, 2) * (width1, height1)\n",
    "    keypoints3 = np.random.rand(num_keypoints, 2) * (width2, height2)\n",
    "    keypoints4 = np.random.rand(num_keypoints, 2) * (width2, height2)\n",
    "\n",
    "    db.add_keypoints(image_id1, keypoints1)\n",
    "    db.add_keypoints(image_id2, keypoints2)\n",
    "    db.add_keypoints(image_id3, keypoints3)\n",
    "    db.add_keypoints(image_id4, keypoints4)\n",
    "\n",
    "    # Create dummy matches.\n",
    "    M = 50\n",
    "    matches12 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "    matches23 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "    matches34 = np.random.randint(num_keypoints, size=(M, 2))\n",
    "\n",
    "    db.add_matches(image_id1, image_id2, matches12)\n",
    "    db.add_matches(image_id2, image_id3, matches23)\n",
    "    db.add_matches(image_id3, image_id4, matches34)\n",
    "\n",
    "    # Commit the data to the file.\n",
    "    db.commit()\n",
    "\n",
    "    # Read and check cameras.\n",
    "    rows = db.execute(\"SELECT * FROM cameras\")\n",
    "\n",
    "    camera_id, model, width, height, params, prior = next(rows)\n",
    "    params = blob_to_array(params, np.float64)\n",
    "    assert camera_id == camera_id1\n",
    "    assert model == model1 and width == width1 and height == height1\n",
    "    assert np.allclose(params, params1)\n",
    "\n",
    "    camera_id, model, width, height, params, prior = next(rows)\n",
    "    params = blob_to_array(params, np.float64)\n",
    "    assert camera_id == camera_id2\n",
    "    assert model == model2 and width == width2 and height == height2\n",
    "    assert np.allclose(params, params2)\n",
    "\n",
    "    # Read and check keypoints.\n",
    "    keypoints = dict(\n",
    "        (image_id, blob_to_array(data, np.float32, (-1, 2)))\n",
    "        for image_id, data in db.execute(\n",
    "            \"SELECT image_id, data FROM keypoints\"))\n",
    "\n",
    "    assert np.allclose(keypoints[image_id1], keypoints1)\n",
    "    assert np.allclose(keypoints[image_id2], keypoints2)\n",
    "    assert np.allclose(keypoints[image_id3], keypoints3)\n",
    "    assert np.allclose(keypoints[image_id4], keypoints4)\n",
    "\n",
    "    # Read and check matches.\n",
    "    pair_ids = [image_ids_to_pair_id(*pair) for pair in\n",
    "                ((image_id1, image_id2),\n",
    "                 (image_id2, image_id3),\n",
    "                 (image_id3, image_id4))]\n",
    "\n",
    "    matches = dict(\n",
    "        (pair_id_to_image_ids(pair_id),\n",
    "         blob_to_array(data, np.uint32, (-1, 2)))\n",
    "        for pair_id, data in db.execute(\"SELECT pair_id, data FROM matches\")\n",
    "    )\n",
    "\n",
    "    assert np.all(matches[(image_id1, image_id2)] == matches12)\n",
    "    assert np.all(matches[(image_id2, image_id3)] == matches23)\n",
    "    assert np.all(matches[(image_id3, image_id4)] == matches34)\n",
    "\n",
    "    # Clean up.\n",
    "    db.close()\n",
    "\n",
    "    if os.path.exists(args.database_path):\n",
    "        os.remove(args.database_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_usage()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf0a0a3",
   "metadata": {
    "papermill": {
     "duration": 0.025095,
     "end_time": "2024-05-21T18:02:17.280581",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.255486",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157cfbd",
   "metadata": {
    "papermill": {
     "duration": 0.025058,
     "end_time": "2024-05-21T18:02:17.330786",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.305728",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üñºÔ∏è Setting Up Image Matching Challenge Environment üöÄ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "1. **`IMC_PATH` Variable**:\n",
    "   - `IMC_PATH` is a string variable that holds the path to the directory where the data for the Image Matching Challenge of 2024 is located.\n",
    "   - This variable is used to specify the location of datasets, images, or other relevant files needed for the Image Matching Challenge tasks.\n",
    "   \n",
    "2. **`DEVICE` Variable**:\n",
    "   - `DEVICE` is a variable that determines the device on which PyTorch operations will be performed.\n",
    "   - It checks if a CUDA-enabled GPU is available using `torch.cuda.is_available()`.\n",
    "   - If a GPU is available, `DEVICE` is set to \"cuda\", indicating that operations should be performed on the GPU for faster computation.\n",
    "   - If a GPU is not available, `DEVICE` is set to \"cpu\", indicating that operations will be performed on the CPU.\n",
    "   - This ensures that the code can adapt to different computing environments, leveraging GPU acceleration when available for faster computation.\n",
    "\n",
    "3. **`clear_output(wait=False)`**:\n",
    "   - This function call is likely used to clear the output of the cell, making the notebook output cleaner.\n",
    "   - The `wait=False` parameter ensures that the output is cleared immediately without waiting for other cell executions to complete.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **PyTorch Device Assignment**:\n",
    "   - Understanding how PyTorch assigns operations to different devices (CPU/GPU).\n",
    "   - **Source**: [PyTorch Documentation - Device Assignment](https://pytorch.org/docs/stable/tensor_attributes.html#torch.torch.device)\n",
    "\n",
    "2. **CUDA Availability Check**:\n",
    "   - Learning how to check if a CUDA-enabled GPU is available in PyTorch.\n",
    "   - **Source**: [PyTorch Forums - CUDA Availability Check](https://discuss.pytorch.org/t/how-to-check-if-pytorch-is-using-the-gpu/311/3)\n",
    "\n",
    "3. **Jupyter `clear_output` Function**:\n",
    "   - Understanding the usage of the `clear_output` function in Jupyter notebooks.\n",
    "   - **Source**: [Jupyter Documentation - Output Clearing](https://ipython.org/ipython-doc/3/api/generated/IPython.display.html#IPython.display.clear_output)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "78f81933",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:17.382672Z",
     "iopub.status.busy": "2024-05-21T18:02:17.382386Z",
     "iopub.status.idle": "2024-05-21T18:02:17.423972Z",
     "shell.execute_reply": "2024-05-21T18:02:17.423217Z"
    },
    "papermill": {
     "duration": 0.070082,
     "end_time": "2024-05-21T18:02:17.426132",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.356050",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "IMC_PATH = '/kaggle/input/image-matching-challenge-2024'\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "clear_output(wait=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517f13bb",
   "metadata": {
    "papermill": {
     "duration": 0.025393,
     "end_time": "2024-05-21T18:02:17.477117",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.451724",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd791405",
   "metadata": {
    "papermill": {
     "duration": 0.024989,
     "end_time": "2024-05-21T18:02:17.527192",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.502203",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîÑ Rotating Image Using PyTorch üñºÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `rotate_image` rotates an image by a multiple of 90 degrees using a PyTorch model for rotation prediction.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `rotate_image(image, rotation)`: This function takes two parameters:\n",
    "     - `image`: The input image tensor to be rotated.\n",
    "     - `rotation`: A PyTorch model that predicts the rotation angle of the input image.\n",
    "\n",
    "2. **Image Rotation Process**:\n",
    "   - The function iterates four times (equivalent to 360 degrees) to cover all possible rotations.\n",
    "   - For each iteration, it uses the provided rotation model to predict the rotation angle of the input image.\n",
    "   - If the predicted rotation angle is 0 (indicating no rotation needed), the loop breaks, and the original image is returned.\n",
    "   - If a non-zero rotation angle is predicted, the image is rotated by 90 degrees clockwise using the `rot90` function.\n",
    "   - This process continues until either a rotation of 0 degrees is predicted or all four iterations are completed.\n",
    "\n",
    "3. **`torch.no_grad()` Context Manager**:\n",
    "   - The `with torch.no_grad():` context manager is used to ensure that no gradient calculations are performed during inference.\n",
    "   - This is beneficial for inference-only operations, as it reduces memory consumption and speeds up computation by avoiding unnecessary gradient tracking.\n",
    "\n",
    "4. **Output**:\n",
    "   - The function returns the rotated image tensor.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Image Rotation in PyTorch**:\n",
    "   - Understanding how to perform image rotation using PyTorch tensors and functions.\n",
    "   - **Source**: [PyTorch Documentation - Image Manipulation](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.functional.rotate)\n",
    "\n",
    "2. **`torch.no_grad()` Context Manager**:\n",
    "   - Learning about the `torch.no_grad()` context manager and its usage in PyTorch.\n",
    "   - **Source**: [PyTorch Documentation - Autograd Mechanics](https://pytorch.org/docs/stable/autograd.html#torch.autograd.no_grad)\n",
    "\n",
    "3. **Rotation Prediction Models**:\n",
    "   - Exploring techniques and models for predicting the rotation angle of images.\n",
    "   - **Source**: [Rotation Prediction Using Deep Learning - Research Paper](https://arxiv.org/abs/1807.02029)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96143294",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:17.578944Z",
     "iopub.status.busy": "2024-05-21T18:02:17.578618Z",
     "iopub.status.idle": "2024-05-21T18:02:17.583786Z",
     "shell.execute_reply": "2024-05-21T18:02:17.582957Z"
    },
    "papermill": {
     "duration": 0.033275,
     "end_time": "2024-05-21T18:02:17.585572",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.552297",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rotate_image(image,rotation):\n",
    "    for i in range(4):\n",
    "        with torch.no_grad():\n",
    "            pred = rotation(image[None,...]).argmax()\n",
    "        if pred == 0: break\n",
    "        image = image.rot90(dims=[1,2])\n",
    "    return image\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1b6108",
   "metadata": {
    "papermill": {
     "duration": 0.025233,
     "end_time": "2024-05-21T18:02:17.636129",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.610896",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984c28f8",
   "metadata": {
    "papermill": {
     "duration": 0.025067,
     "end_time": "2024-05-21T18:02:17.686390",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.661323",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üîç Image Overlap Detection with Feature Matching üñºÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `overlap_detection` detects the overlap region between two images using feature extraction and matching techniques.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `overlap_detection(extractor, matcher, image0, image1, min_matches)`: This function takes five parameters:\n",
    "     - `extractor`: A feature extractor model used to extract features from the images.\n",
    "     - `matcher`: A feature matcher model used to match features between the images.\n",
    "     - `image0`: The first input image tensor.\n",
    "     - `image1`: The second input image tensor.\n",
    "     - `min_matches`: The minimum number of matches required to consider the overlap valid.\n",
    "\n",
    "2. **Feature Extraction and Matching**:\n",
    "   - The function first extracts features from both input images using the provided `extractor` model.\n",
    "   - It then matches the extracted features between the two images using the `matcher` model.\n",
    "   - If the number of matches is less than `min_matches`, the function returns the extracted features and matches without further processing.\n",
    "\n",
    "3. **Bounding Box Calculation**:\n",
    "   - If the number of matches is sufficient, the function calculates the bounding boxes for the matched keypoints in both images.\n",
    "   - It finds the minimum and maximum coordinates of the matched keypoints in each image and calculates the width and height of the bounding box.\n",
    "\n",
    "4. **Image Cropping**:\n",
    "   - Using the calculated bounding boxes, the function crops the overlapping regions from both images.\n",
    "   - It then calls the `match_pair` function (not provided here) to re-match features in the cropped regions.\n",
    "\n",
    "5. **Adjusting Keypoint Coordinates**:\n",
    "   - After matching features in the cropped regions, the function adjusts the coordinates of the keypoints to match the original image coordinates.\n",
    "\n",
    "6. **Output**:\n",
    "   - The function returns the extracted features (`feats0_c` and `feats1_c`) and matches (`matches01_c`) in the cropped regions.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Feature Extraction and Matching in Computer Vision**:\n",
    "   - Understanding the process of feature extraction and matching for image analysis tasks.\n",
    "   - **Source**: [Feature Extraction and Matching Techniques - Computer Vision Foundation](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Schroff_FaceNet_A_Unified_CVPR_2015_paper.pdf)\n",
    "\n",
    "2. **Bounding Box Calculation**:\n",
    "   - Learning about techniques to calculate bounding boxes for objects or regions of interest in images.\n",
    "   - **Source**: [Bounding Box Calculation - Towards Data Science](https://towardsdatascience.com/bounding-box-detection-challenges-and-techniques-dac34cf2de40)\n",
    "\n",
    "3. **Image Cropping in PyTorch**:\n",
    "   - Exploring methods to crop regions of interest from images using PyTorch.\n",
    "   - **Source**: [PyTorch Documentation - Image Transformations](https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.functional.crop)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05093ef4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:17.738833Z",
     "iopub.status.busy": "2024-05-21T18:02:17.738518Z",
     "iopub.status.idle": "2024-05-21T18:02:17.748492Z",
     "shell.execute_reply": "2024-05-21T18:02:17.747642Z"
    },
    "papermill": {
     "duration": 0.03902,
     "end_time": "2024-05-21T18:02:17.750477",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.711457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def overlap_detection(extractor, matcher, image0, image1, min_matches):\n",
    "    feats0, feats1, matches01 = match_pair(extractor, matcher, image0, image1)\n",
    "    if len(matches01['matches']) < min_matches:\n",
    "        return feats0, feats1, matches01\n",
    "    kpts0, kpts1, matches = feats0[\"keypoints\"], feats1[\"keypoints\"], matches01[\"matches\"]\n",
    "    m_kpts0, m_kpts1 = kpts0[matches[..., 0]], kpts1[matches[..., 1]]\n",
    "    left0, top0 = m_kpts0.numpy().min(axis=0).astype(int)\n",
    "    width0, height0 = m_kpts0.numpy().max(axis=0).astype(int)\n",
    "    height0 -= top0\n",
    "    width0 -= left0\n",
    "    left1, top1 = m_kpts1.numpy().min(axis=0).astype(int)\n",
    "    width1, height1 = m_kpts1.numpy().max(axis=0).astype(int)\n",
    "    height1 -= top1\n",
    "    width1 -= left1\n",
    "    crop_box0 = (top0, left0, height0, width0)\n",
    "    crop_box1 = (top1, left1, height1, width1)\n",
    "    cropped_img_tensor0 = TF.crop(image0, *crop_box0)\n",
    "    cropped_img_tensor1 = TF.crop(image1, *crop_box1)\n",
    "    feats0_c, feats1_c, matches01_c = match_pair(extractor, matcher, cropped_img_tensor0, cropped_img_tensor1)\n",
    "    feats0_c['keypoints'][...,0] += left0\n",
    "    feats0_c['keypoints'][...,1] += top0\n",
    "    feats1_c['keypoints'][...,0] += left1\n",
    "    feats1_c['keypoints'][...,1] += top1\n",
    "    return feats0_c, feats1_c, matches01_c\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63da02c",
   "metadata": {
    "papermill": {
     "duration": 0.026039,
     "end_time": "2024-05-21T18:02:17.802611",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.776572",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64368898",
   "metadata": {
    "papermill": {
     "duration": 0.025918,
     "end_time": "2024-05-21T18:02:17.854652",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.828734",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üå± Seed Resetting for Reproducibility üîÑ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `reset_seed` resets the random seed for PyTorch and NumPy to ensure reproducibility in machine learning experiments.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `reset_seed(seed)`: This function takes one parameter:\n",
    "     - `seed`: The value of the seed to reset the random number generators.\n",
    "\n",
    "2. **Resetting PyTorch Seed**:\n",
    "   - `torch.manual_seed(seed)`: Sets the seed for generating random numbers in PyTorch on the CPU.\n",
    "   - `torch.cuda.manual_seed_all(seed)`: Sets the seed for generating random numbers in PyTorch on all available GPUs.\n",
    "\n",
    "3. **Resetting NumPy Seed**:\n",
    "   - `np.random.seed(seed)`: Sets the seed for generating random numbers in NumPy.\n",
    "\n",
    "4. **Importance of Seed Resetting**:\n",
    "   - Resetting the random seed ensures that the same sequence of random numbers is generated each time the code is run with the same seed.\n",
    "   - This is crucial for reproducibility in machine learning experiments, as it allows researchers to obtain consistent results across different runs.\n",
    "\n",
    "5. **Usage**:\n",
    "   - Call this function at the beginning of an experiment or before running any code that involves random number generation to ensure reproducibility.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Random Seed and Reproducibility**:\n",
    "   - Understanding the importance of setting random seeds for reproducible results in machine learning experiments.\n",
    "   - **Source**: [Reproducible Research and Random Seeds - Medium](https://medium.com/acing-ai/reproducible-research-and-random-seeds-5876a1f34fc0)\n",
    "\n",
    "2. **PyTorch Seed Setting**:\n",
    "   - Learning about setting random seeds in PyTorch for reproducibility.\n",
    "   - **Source**: [PyTorch Documentation - Randomness](https://pytorch.org/docs/stable/notes/randomness.html)\n",
    "\n",
    "3. **NumPy Random Seed**:\n",
    "   - Exploring how to set random seeds in NumPy for reproducibility.\n",
    "   - **Source**: [NumPy Documentation - Random Sampling](https://numpy.org/doc/stable/reference/random/index.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a79f342",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:17.907990Z",
     "iopub.status.busy": "2024-05-21T18:02:17.907453Z",
     "iopub.status.idle": "2024-05-21T18:02:17.911967Z",
     "shell.execute_reply": "2024-05-21T18:02:17.911084Z"
    },
    "papermill": {
     "duration": 0.032689,
     "end_time": "2024-05-21T18:02:17.913903",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.881214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def reset_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6825c2c",
   "metadata": {
    "papermill": {
     "duration": 0.026127,
     "end_time": "2024-05-21T18:02:17.965624",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.939497",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ca4f91",
   "metadata": {
    "papermill": {
     "duration": 0.025597,
     "end_time": "2024-05-21T18:02:18.017457",
     "exception": false,
     "start_time": "2024-05-21T18:02:17.991860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìÑ Parsing Sample Submission Data üìä\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `parse_sample_submission` parses the sample submission data from a given file and organizes it into a dictionary for further analysis.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `parse_sample_submission(data_path)`: This function takes one parameter:\n",
    "     - `data_path`: The path to the sample submission data file.\n",
    "\n",
    "2. **Parsing Sample Submission Data**:\n",
    "   - The function reads the content of the file line by line.\n",
    "   - It skips the first line (header) and prints it for reference.\n",
    "   - For each subsequent line, it extracts information such as `image_path`, `dataset`, and `scene`.\n",
    "   - It constructs a dictionary `data_dict` to store the parsed data. The dictionary is organized by `dataset` and `scene`.\n",
    "   - Each `dataset` contains multiple `scenes`, and each `scene` contains a list of image paths.\n",
    "   - The image paths are converted to `Path` objects for convenient manipulation.\n",
    "\n",
    "3. **Printing Dataset Information**:\n",
    "   - After parsing, the function prints the number of images for each `dataset` and `scene` combination.\n",
    "\n",
    "4. **Returning Data Dictionary**:\n",
    "   - Finally, the function returns the parsed data dictionary `data_dict`.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **File Parsing in Python**:\n",
    "   - Understanding how to read and parse data from files in Python.\n",
    "   - **Source**: [Python Documentation - File Input and Output](https://docs.python.org/3/tutorial/inputoutput.html#reading-and-writing-files)\n",
    "\n",
    "2. **Dictionary Data Structure**:\n",
    "   - Learning about dictionaries in Python and how to organize data using key-value pairs.\n",
    "   - **Source**: [Real Python - Dictionaries in Python](https://realpython.com/python-dicts/)\n",
    "\n",
    "3. **Pathlib Module**:\n",
    "   - Exploring the `Pathlib` module for working with file paths in Python.\n",
    "   - **Source**: [Python Documentation - pathlib](https://docs.python.org/3/library/pathlib.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d3e6c9cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.070424Z",
     "iopub.status.busy": "2024-05-21T18:02:18.069662Z",
     "iopub.status.idle": "2024-05-21T18:02:18.077124Z",
     "shell.execute_reply": "2024-05-21T18:02:18.076272Z"
    },
    "papermill": {
     "duration": 0.036153,
     "end_time": "2024-05-21T18:02:18.079044",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.042891",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def parse_sample_submission(data_path):\n",
    "    data_dict = {}\n",
    "    with open(data_path, \"r\") as f:\n",
    "        for i, l in enumerate(f):\n",
    "            if i == 0:\n",
    "                print(\"header:\", l)\n",
    "\n",
    "            if l and i > 0:\n",
    "                image_path, dataset, scene, _, _ = l.strip().split(',')\n",
    "                if dataset not in data_dict:\n",
    "                    data_dict[dataset] = {}\n",
    "                if scene not in data_dict[dataset]:\n",
    "                    data_dict[dataset][scene] = []\n",
    "                data_dict[dataset][scene].append(Path(IMC_PATH +'/'+ image_path))\n",
    "\n",
    "    for dataset in data_dict:\n",
    "        for scene in data_dict[dataset]:\n",
    "            print(f\"{dataset} / {scene} -> {len(data_dict[dataset][scene])} images\")\n",
    "\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4d3a109",
   "metadata": {
    "papermill": {
     "duration": 0.025334,
     "end_time": "2024-05-21T18:02:18.129897",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.104563",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbcc965",
   "metadata": {
    "papermill": {
     "duration": 0.025554,
     "end_time": "2024-05-21T18:02:18.181196",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.155642",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìù Converting Array to String Representation üîÑ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `arr_to_str` converts a NumPy array to a string representation by flattening it and joining its elements.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `arr_to_str(a)`: This function takes one parameter:\n",
    "     - `a`: The input NumPy array to be converted to a string.\n",
    "\n",
    "2. **Flattening the Array**:\n",
    "   - The function first reshapes the input array `a` using `.reshape(-1)`, which flattens the array into a 1-dimensional shape.\n",
    "   - This ensures that regardless of the input array's dimensions, it will be converted to a 1D array.\n",
    "\n",
    "3. **Joining Array Elements**:\n",
    "   - The flattened array elements are then converted to strings using a list comprehension `[str(x) for x in a.reshape(-1)]`.\n",
    "   - Each element is converted to a string to prepare for joining.\n",
    "\n",
    "4. **Joining with Separator**:\n",
    "   - The `join` method is used to concatenate the string representations of array elements into a single string.\n",
    "   - A semicolon (`;`) is used as the separator to distinguish between individual elements.\n",
    "\n",
    "5. **Returning String Representation**:\n",
    "   - The function returns the concatenated string representation of the input array.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f5181132",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.234031Z",
     "iopub.status.busy": "2024-05-21T18:02:18.233359Z",
     "iopub.status.idle": "2024-05-21T18:02:18.237653Z",
     "shell.execute_reply": "2024-05-21T18:02:18.236833Z"
    },
    "papermill": {
     "duration": 0.032807,
     "end_time": "2024-05-21T18:02:18.239619",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.206812",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def arr_to_str(a):\n",
    "    return \";\".join([str(x) for x in a.reshape(-1)])\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9709f72",
   "metadata": {
    "papermill": {
     "duration": 0.025872,
     "end_time": "2024-05-21T18:02:18.291279",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.265407",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516f4f57",
   "metadata": {
    "papermill": {
     "duration": 0.025645,
     "end_time": "2024-05-21T18:02:18.342730",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.317085",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üìÑ Creating Submission File from Results üñãÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `create_submission` generates a submission CSV file based on the results obtained from processing the input data.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `create_submission(results, data_dict, base_path)`: This function takes three parameters:\n",
    "     - `results`: A dictionary containing the processed results, typically obtained from some analysis or computation.\n",
    "     - `data_dict`: A dictionary containing information about the dataset and scenes.\n",
    "     - `base_path`: The base path to be used for constructing relative paths of image files.\n",
    "\n",
    "2. **Writing Header**:\n",
    "   - The function opens the submission file in write mode and writes the header line containing column names: `\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\"`.\n",
    "\n",
    "3. **Iterating Over Datasets and Scenes**:\n",
    "   - It iterates over each dataset in the `data_dict`.\n",
    "   - For each dataset, it checks if corresponding results are available in the `results` dictionary. If not, it initializes an empty dictionary.\n",
    "   - It then iterates over each scene in the dataset and checks if scene-specific results are available. If not, it initializes empty dictionaries for rotation (`R`) and translation (`t`).\n",
    "\n",
    "4. **Writing Data to CSV**:\n",
    "   - For each image in the dataset and scene, it retrieves the rotation matrix (`R`) and translation vector (`t`) from the results. If not available, it defaults to identity matrix and zero vector, respectively.\n",
    "   - It constructs the relative image path with respect to the `base_path`.\n",
    "   - It writes the image path, dataset, scene, rotation matrix, and translation vector to the CSV file.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **CSV File Writing in Python**:\n",
    "   - Understanding how to write data to CSV files using Python.\n",
    "   - **Source**: [Real Python - Reading and Writing CSV Files](https://realpython.com/python-csv/)\n",
    "\n",
    "2. **Dictionary Manipulation in Python**:\n",
    "   - Learning about techniques for working with dictionaries in Python, including iteration and key-value access.\n",
    "   - **Source**: [Python Documentation - Dictionaries](https://docs.python.org/3/tutorial/datastructures.html#dictionaries)\n",
    "\n",
    "3. **Pathlib Module**:\n",
    "   - Exploring the `Pathlib` module for working with file paths in Python.\n",
    "   - **Source**: [Python Documentation - pathlib](https://docs.python.org/3/library/pathlib.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b59930a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.395584Z",
     "iopub.status.busy": "2024-05-21T18:02:18.395263Z",
     "iopub.status.idle": "2024-05-21T18:02:18.403565Z",
     "shell.execute_reply": "2024-05-21T18:02:18.402655Z"
    },
    "papermill": {
     "duration": 0.037423,
     "end_time": "2024-05-21T18:02:18.405625",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.368202",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def create_submission(results,data_dict,base_path):    \n",
    "    with open(\"submission.csv\", \"w\") as f:\n",
    "        f.write(\"image_path,dataset,scene,rotation_matrix,translation_vector\\n\")\n",
    "        \n",
    "        for dataset in data_dict:\n",
    "            if dataset in results:\n",
    "                res = results[dataset]\n",
    "            else:\n",
    "                res = {}\n",
    "            \n",
    "            for scene in data_dict[dataset]:\n",
    "                if scene in res:\n",
    "                    scene_res = res[scene]\n",
    "                else:\n",
    "                    scene_res = {\"R\":{}, \"t\":{}}\n",
    "                    \n",
    "                for image in data_dict[dataset][scene]:\n",
    "                    if image in scene_res:\n",
    "                        R = scene_res[image][\"R\"].reshape(-1)\n",
    "                        T = scene_res[image][\"t\"].reshape(-1)\n",
    "                    else:\n",
    "                        R = np.eye(3).reshape(-1)\n",
    "                        T = np.zeros((3))\n",
    "                    image_path = str(image.relative_to(base_path))\n",
    "                    f.write(f\"{image_path},{dataset},{scene},{arr_to_str(R)},{arr_to_str(T)}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05660c3f",
   "metadata": {
    "papermill": {
     "duration": 0.02586,
     "end_time": "2024-05-21T18:02:18.457318",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.431458",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89459b3d",
   "metadata": {
    "papermill": {
     "duration": 0.025815,
     "end_time": "2024-05-21T18:02:18.509563",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.483748",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# üöÄ Running Image Matching Pipeline üñºÔ∏è\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `run` orchestrates the entire image matching pipeline, from data loading to submission creation.\n",
    "\n",
    "1. **Function Signature**:\n",
    "   - `run(data_path, get_pairs, keypoints_matches, ransac_and_sparse_reconstruction, submit=True)`: This function takes five parameters:\n",
    "     - `data_path`: The path to the sample submission data file.\n",
    "     - `get_pairs`: Function for obtaining pairs of images for matching.\n",
    "     - `keypoints_matches`: Function for extracting keypoints and computing matches between images.\n",
    "     - `ransac_and_sparse_reconstruction`: Function for performing RANSAC and sparse reconstruction.\n",
    "     - `submit`: Boolean flag indicating whether to generate a submission file (default is `True`).\n",
    "\n",
    "2. **Parsing Sample Submission Data**:\n",
    "   - The function calls `parse_sample_submission` to parse the sample submission data and store it in the `data_dict` variable.\n",
    "\n",
    "3. **Iterating Over Datasets and Scenes**:\n",
    "   - It iterates over each dataset and scene in the parsed data.\n",
    "   - For each dataset-scene pair, it retrieves the image paths and initializes the `results` dictionary to store the processed results.\n",
    "\n",
    "4. **Running Image Matching Pipeline**:\n",
    "   - It calls the provided functions `get_pairs`, `keypoints_matches`, and `ransac_and_sparse_reconstruction` to perform various steps of the image matching pipeline.\n",
    "   - These functions are responsible for obtaining image pairs, extracting keypoints and computing matches, and performing RANSAC and sparse reconstruction, respectively.\n",
    "\n",
    "5. **Generating Submission**:\n",
    "   - If `submit` flag is `True`, it generates a submission file using the `create_submission` function with the processed results.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Function Composition in Python**:\n",
    "   - Understanding how to compose functions together to build complex workflows in Python.\n",
    "   - **Source**: [Real Python - Function Composition](https://realpython.com/python-functional-programming/)\n",
    "\n",
    "2. **Data Pipelines in Machine Learning**:\n",
    "   - Learning about the concept of data pipelines and how they are used to organize and execute machine learning workflows.\n",
    "   - **Source**: [Towards Data Science - Building Machine Learning Pipelines](https://towardsdatascience.com/building-machine-learning-pipelines-b86f5f12f7eb)\n",
    "\n",
    "3. **Image Matching and Reconstruction**:\n",
    "   - Exploring the techniques and algorithms involved in image matching, keypoint extraction, feature matching, and sparse reconstruction.\n",
    "   - **Source**: [OpenCV Documentation - Feature Detection and Description](https://docs.opencv.org/4.x/db/d27/tutorial_py_table_of_contents_feature2d.html)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b723f65a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.562525Z",
     "iopub.status.busy": "2024-05-21T18:02:18.561838Z",
     "iopub.status.idle": "2024-05-21T18:02:18.571832Z",
     "shell.execute_reply": "2024-05-21T18:02:18.570998Z"
    },
    "papermill": {
     "duration": 0.03855,
     "end_time": "2024-05-21T18:02:18.573710",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.535160",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def run(data_path,get_pairs,keypoints_matches,ransac_and_sparse_reconstruction,submit=True):\n",
    "    results = {}\n",
    "    \n",
    "    data_dict = parse_sample_submission(data_path)\n",
    "    datasets = list(data_dict.keys())\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        if dataset not in results:\n",
    "            results[dataset] = {}\n",
    "            \n",
    "        for scene in data_dict[dataset]:\n",
    "            images_dir = data_dict[dataset][scene][0].parent\n",
    "            results[dataset][scene] = {}\n",
    "            image_paths = data_dict[dataset][scene]\n",
    "\n",
    "            index_pairs = get_pairs(image_paths)\n",
    "            keypoints_matches(image_paths,index_pairs)                \n",
    "            maps = ransac_and_sparse_reconstruction(image_paths[0].parent)\n",
    "            clear_output(wait=False)\n",
    "            \n",
    "            path = 'test' if submit else 'train'\n",
    "            images_registered  = 0\n",
    "            best_idx = 0\n",
    "            for idx, rec in maps.items():\n",
    "                if len(rec.images) > images_registered:\n",
    "                    images_registered = len(rec.images)\n",
    "                    best_idx = idx\n",
    "                    \n",
    "            for k, im in maps[best_idx].images.items():\n",
    "                key = Path(IMC_PATH) / path / scene / \"images\" / im.name\n",
    "                results[dataset][scene][key] = {}\n",
    "                results[dataset][scene][key][\"R\"] = deepcopy(im.cam_from_world.rotation.matrix())\n",
    "                results[dataset][scene][key][\"t\"] = deepcopy(np.array(im.cam_from_world.translation))\n",
    "\n",
    "            create_submission(results, data_dict, Path(IMC_PATH))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc3688",
   "metadata": {
    "papermill": {
     "duration": 0.025827,
     "end_time": "2024-05-21T18:02:18.625677",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.599850",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac7f58bc",
   "metadata": {
    "papermill": {
     "duration": 0.025474,
     "end_time": "2024-05-21T18:02:18.676671",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.651197",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Constants for Translation Thresholds üìè\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "Define a dictionary `translation_thresholds_meters_dict` that maps scene names to arrays of translation thresholds in meters. These thresholds are typically used for evaluating the accuracy of the estimated translations between images in a scene.\n",
    "\n",
    "1. **Translation Thresholds Dictionary**:\n",
    "   - The dictionary `translation_thresholds_meters_dict` contains scene names as keys and corresponding arrays of translation thresholds as values.\n",
    "   - Each scene may have different thresholds depending on factors such as scene complexity or expected level of accuracy.\n",
    "\n",
    "2. **Scene Names and Translation Thresholds**:\n",
    "   - Each scene is associated with an array of translation thresholds, which are specified in meters.\n",
    "   - The thresholds are typically chosen based on the scale of the scene and the desired level of precision in the estimated translations.\n",
    "\n",
    "3. **EPS (Epsilon) Value**:\n",
    "   - The `_EPS` constant is defined as a small value that is used to handle numerical precision issues.\n",
    "   - It is calculated as the machine epsilon multiplied by 4.0, which provides a small margin for comparison operations involving floating-point numbers.\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Numerical Precision and Machine Epsilon**:\n",
    "   - Understanding the concept of machine epsilon and its importance in handling numerical precision in floating-point arithmetic.\n",
    "   - **Source**: [Wikipedia - Machine Epsilon](https://en.wikipedia.org/wiki/Machine_epsilon)\n",
    "\n",
    "2. **Evaluation Metrics in Computer Vision**:\n",
    "   - Learning about common evaluation metrics used in computer vision tasks, including thresholds for translation accuracy.\n",
    "   - **Source**: [Computer Vision: Metrics and Evaluation](https://learnopencv.com/computer-vision-metrics-and-evaluation/)\n",
    "\n",
    "3. **Scene Complexity and Accuracy Requirements**:\n",
    "   - Understanding how scene complexity and desired accuracy influence the choice of evaluation thresholds in image processing and computer vision tasks.\n",
    "   - **Source**: [Computer Vision: Algorithms and Applications](https://www.amazon.com/Computer-Vision-Algorithms-Applications-Richard/dp/1439867998)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "889bd24f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.729469Z",
     "iopub.status.busy": "2024-05-21T18:02:18.728869Z",
     "iopub.status.idle": "2024-05-21T18:02:18.735882Z",
     "shell.execute_reply": "2024-05-21T18:02:18.735037Z"
    },
    "papermill": {
     "duration": 0.035608,
     "end_time": "2024-05-21T18:02:18.737826",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.702218",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "_EPS = np.finfo(float).eps * 4.0\n",
    "translation_thresholds_meters_dict = {\n",
    " 'multi-temporal-temple-baalshamin':  np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'pond':                              np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'transp_obj_glass_cylinder':         np.array([0.0025, 0.005, 0.01, 0.02, 0.05, 0.1]),\n",
    " 'transp_obj_glass_cup':              np.array([0.0025, 0.005, 0.01, 0.02, 0.05, 0.1]),\n",
    " 'church':                            np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'lizard':                            np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]),\n",
    " 'dioscuri':                          np.array([0.025,  0.05,  0.1,  0.2,  0.5,  1.0]), \n",
    "}\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b13483e",
   "metadata": {
    "papermill": {
     "duration": 0.026652,
     "end_time": "2024-05-21T18:02:18.790108",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.763456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6494346",
   "metadata": {
    "papermill": {
     "duration": 0.02613,
     "end_time": "2024-05-21T18:02:18.842204",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.816074",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Vector Norm Calculation üìê\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function `vector_norm` computes the Euclidean norm (or length) of a numpy array. The Euclidean norm is the standard length of a vector and is computed as the square root of the sum of the squared elements. This function can handle both 1-dimensional and multi-dimensional arrays, and allows specifying the axis along which to compute the norm.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `vector_norm` takes three parameters:\n",
    "     - `data`: The input array whose norm is to be calculated.\n",
    "     - `axis`: The axis along which to compute the norm. If `None`, the norm is computed over the entire array.\n",
    "     - `out`: An optional output array to store the result.\n",
    "\n",
    "2. **Conversion to Numpy Array**:\n",
    "   - The input `data` is converted to a numpy array of type `float64` to ensure precision in calculations.\n",
    "\n",
    "3. **Handling 1D Arrays**:\n",
    "   - If the input array is 1-dimensional and `out` is `None`, the norm is calculated using `np.dot` to compute the dot product of the array with itself, followed by taking the square root.\n",
    "\n",
    "4. **Handling Multi-Dimensional Arrays**:\n",
    "   - For multi-dimensional arrays, the elements of the array are squared, and the sum is computed along the specified axis.\n",
    "   - If `out` is `None`, the sum of the squared elements is stored in a new array, `out`, and the square root of this array is returned.\n",
    "   - If `out` is provided, the result is stored in the provided output array after summing the squared elements and taking the square root.\n",
    "\n",
    "5. **In-Place Operations**:\n",
    "   - The operations are performed in-place when `out` is provided to avoid creating additional copies of the data, making the function more memory efficient.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example 1: 1D array\n",
    "vector = np.array([3, 4])\n",
    "print(vector_norm(vector))  # Output: 5.0\n",
    "\n",
    "# Example 2: 2D array, norm along rows (axis=1)\n",
    "matrix = np.array([[1, 2, 3], [4, 5, 6]])\n",
    "print(vector_norm(matrix, axis=1))  # Output: [3.74165739 8.77496439]\n",
    "```\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Numpy Documentation**:\n",
    "   - Understanding numpy array operations and functions such as `np.dot`, `np.sum`, and `np.sqrt`.\n",
    "   - **Source**: [Numpy Documentation](https://numpy.org/doc/stable/)\n",
    "\n",
    "2. **Vector Norms in Linear Algebra**:\n",
    "   - Learning about different types of vector norms and their applications in linear algebra and machine learning.\n",
    "   - **Source**: [Linear Algebra - Norms](https://www.math.ucla.edu/~yanovsky/Teaching/Math151B/handouts/Norms.pdf)\n",
    "\n",
    "3. **In-Place Operations in Numpy**:\n",
    "   - Understanding in-place operations and their benefits in terms of memory efficiency.\n",
    "   - **Source**: [In-Place Operations in Numpy](https://numpy.org/doc/stable/reference/generated/numpy.ufunc.at.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9b2ed548",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:18.897251Z",
     "iopub.status.busy": "2024-05-21T18:02:18.896893Z",
     "iopub.status.idle": "2024-05-21T18:02:18.903549Z",
     "shell.execute_reply": "2024-05-21T18:02:18.902670Z"
    },
    "papermill": {
     "duration": 0.037197,
     "end_time": "2024-05-21T18:02:18.905429",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.868232",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def vector_norm(data, axis=None, out=None):\n",
    "    '''Return length, i.e. Euclidean norm, of ndarray along axis.'''\n",
    "    data = np.array(data, dtype=np.float64, copy=True)\n",
    "    if out is None:\n",
    "        if data.ndim == 1:\n",
    "            return math.sqrt(np.dot(data, data))\n",
    "        data *= data\n",
    "        out = np.atleast_1d(np.sum(data, axis=axis))\n",
    "        np.sqrt(out, out)\n",
    "        return out\n",
    "    data *= data\n",
    "    np.sum(data, axis=axis, out=out)\n",
    "    np.sqrt(out, out)\n",
    "    return None\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161c9c6c",
   "metadata": {
    "papermill": {
     "duration": 0.026464,
     "end_time": "2024-05-21T18:02:18.957862",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.931398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc509dc3",
   "metadata": {
    "papermill": {
     "duration": 0.026146,
     "end_time": "2024-05-21T18:02:19.010018",
     "exception": false,
     "start_time": "2024-05-21T18:02:18.983872",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Quaternion to Homogeneous Rotation Matrix Conversion üîÑ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function, `quaternion_matrix`, converts a quaternion into a homogeneous rotation matrix. Quaternions are often used in computer graphics, robotics, and aerospace for representing rotations because they are more numerically stable and efficient compared to other representations like Euler angles.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `quaternion_matrix` takes a single parameter:\n",
    "     - `quaternion`: A 4-element array representing the quaternion (q0, q1, q2, q3).\n",
    "\n",
    "2. **Conversion to Numpy Array**:\n",
    "   - The input quaternion is converted to a numpy array of type `float64` for precision.\n",
    "\n",
    "3. **Normalization Check**:\n",
    "   - The norm (squared magnitude) of the quaternion is computed.\n",
    "   - If the norm is very small (less than a tiny threshold `_EPS`), the function returns the identity matrix. This is a special case to handle quaternions that are essentially zero, which represents no rotation.\n",
    "\n",
    "4. **Normalization and Scaling**:\n",
    "   - The quaternion is scaled by the square root of \\(2/n\\) to ensure it is normalized. This scaling factor ensures that the resulting rotation matrix is valid.\n",
    "\n",
    "5. **Outer Product Calculation**:\n",
    "   - The outer product of the quaternion with itself is computed to form a 4x4 matrix. This matrix is used in the construction of the rotation matrix.\n",
    "\n",
    "6. **Rotation Matrix Construction**:\n",
    "   - The elements of the 4x4 homogeneous rotation matrix are populated using the elements of the outer product matrix. This matrix includes both the rotation (top-left 3x3 submatrix) and homogeneous transformation (last column and row).\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example quaternion (unit quaternion representing no rotation)\n",
    "quaternion = [1, 0, 0, 0]\n",
    "\n",
    "# Convert quaternion to rotation matrix\n",
    "rotation_matrix = quaternion_matrix(quaternion)\n",
    "print(rotation_matrix)\n",
    "```\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Quaternions in 3D Rotations**:\n",
    "   - Understanding quaternions and their use in representing rotations in 3D space.\n",
    "   - **Source**: [Quaternions and spatial rotation](https://en.wikipedia.org/wiki/Quaternions_and_spatial_rotation)\n",
    "\n",
    "2. **Homogeneous Transformation Matrices**:\n",
    "   - Learning about homogeneous coordinates and transformation matrices in computer graphics and robotics.\n",
    "   - **Source**: [Homogeneous Transformation Matrices](https://www.mathworks.com/help/robotics/ug/homogeneous-transformations.html)\n",
    "\n",
    "3. **Numerical Stability in Quaternion Operations**:\n",
    "   - The importance of numerical stability when working with quaternions, especially in computational applications.\n",
    "   - **Source**: [Quaternion Normalization](https://www.oreilly.com/library/view/3d-math-primer/9781568817231/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63256492",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:19.063661Z",
     "iopub.status.busy": "2024-05-21T18:02:19.062913Z",
     "iopub.status.idle": "2024-05-21T18:02:19.071548Z",
     "shell.execute_reply": "2024-05-21T18:02:19.070703Z"
    },
    "papermill": {
     "duration": 0.037807,
     "end_time": "2024-05-21T18:02:19.073577",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.035770",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def quaternion_matrix(quaternion):\n",
    "    '''Return homogeneous rotation matrix from quaternion.'''\n",
    "    q = np.array(quaternion, dtype=np.float64, copy=True)\n",
    "    n = np.dot(q, q)\n",
    "    if n < _EPS:\n",
    "        # print(\"special case\")\n",
    "        return np.identity(4)\n",
    "    q *= math.sqrt(2.0 / n)\n",
    "    q = np.outer(q, q)\n",
    "    return np.array(\n",
    "        [\n",
    "            [\n",
    "                1.0 - q[2, 2] - q[3, 3],\n",
    "                q[1, 2] - q[3, 0],\n",
    "                q[1, 3] + q[2, 0],\n",
    "                0.0,\n",
    "            ],\n",
    "            [\n",
    "                q[1, 2] + q[3, 0],\n",
    "                1.0 - q[1, 1] - q[3, 3],\n",
    "                q[2, 3] - q[1, 0],\n",
    "                0.0,\n",
    "            ],\n",
    "            [\n",
    "                q[1, 3] - q[2, 0],\n",
    "                q[2, 3] + q[1, 0],\n",
    "                1.0 - q[1, 1] - q[2, 2],\n",
    "                0.0,\n",
    "            ],\n",
    "            [0.0, 0.0, 0.0, 1.0],\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c0a02",
   "metadata": {
    "papermill": {
     "duration": 0.02592,
     "end_time": "2024-05-21T18:02:19.125533",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.099613",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4afe85fc",
   "metadata": {
    "papermill": {
     "duration": 0.025801,
     "end_time": "2024-05-21T18:02:19.177226",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.151425",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Affine Transformation Matrix from Point Correspondences ‚ú®\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function, `affine_matrix_from_points`, computes an affine transformation matrix that maps points in `v0` to corresponding points in `v1`. Affine transformations include translation, scaling, rotation, and shearing. The function supports options for including or excluding shearing and scaling in the transformation.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `affine_matrix_from_points` takes the following parameters:\n",
    "     - `v0`: A set of source points.\n",
    "     - `v1`: A set of destination points.\n",
    "     - `shear`: A boolean flag to include/exclude shearing in the transformation.\n",
    "     - `scale`: A boolean flag to include/exclude scaling in the transformation.\n",
    "     - `usesvd`: A boolean flag to use Singular Value Decomposition (SVD) for the transformation.\n",
    "\n",
    "2. **Input Validation**:\n",
    "   - The function checks if the input arrays have the correct shape and type. If the conditions are not met, it raises a `ValueError`.\n",
    "\n",
    "3. **Moving Centroids to Origin**:\n",
    "   - The centroids of `v0` and `v1` are calculated and subtracted from the points to move the centroids to the origin. This step simplifies the computation of the transformation matrix.\n",
    "   - Transformation matrices `M0` and `M1` are created to move the centroids back to their original positions later.\n",
    "\n",
    "4. **Affine Transformation**:\n",
    "   - If `shear` is enabled, the function computes the affine transformation using SVD on the concatenated matrices of `v0` and `v1`.\n",
    "\n",
    "5. **Rigid Transformation via SVD**:\n",
    "   - If `shear` is disabled or `usesvd` is enabled, the function computes the rigid transformation using SVD on the covariance matrix of `v1` and `v0`. The resulting rotation matrix `R` is used to form the transformation matrix `M`.\n",
    "\n",
    "6. **Rigid Transformation via Quaternion**:\n",
    "   - If `usesvd` is disabled and the dimension is not 3, the function computes the rigid transformation using quaternion. It constructs a symmetric matrix `N`, computes its eigenvalues and eigenvectors, and forms the rotation matrix from the unit quaternion.\n",
    "\n",
    "7. **Scaling**:\n",
    "   - If `scale` is enabled and `shear` is disabled, the function applies scaling by adjusting the transformation matrix based on the ratio of RMS deviations from the centroids of `v0` and `v1`.\n",
    "\n",
    "8. **Moving Centroids Back**:\n",
    "   - The function moves the centroids back to their original positions by multiplying the transformation matrices `M0` and `M1`.\n",
    "\n",
    "9. **Normalization**:\n",
    "   - The transformation matrix `M` is normalized to ensure the last element is 1.\n",
    "\n",
    "10. **Return**:\n",
    "    - The function returns the final transformation matrix `M`.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example source and destination points\n",
    "v0 = np.array([[0, 1, 2], [0, 1, 2]])\n",
    "v1 = np.array([[1, 2, 3], [1, 2, 3]])\n",
    "\n",
    "# Compute affine transformation matrix\n",
    "transformation_matrix = affine_matrix_from_points(v0, v1)\n",
    "print(transformation_matrix)\n",
    "```\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **Affine Transformations**:\n",
    "   - Understanding the mathematical foundation and applications of affine transformations in computer vision and graphics.\n",
    "   - **Source**: [Affine Transformation](https://en.wikipedia.org/wiki/Affine_transformation)\n",
    "\n",
    "2. **Singular Value Decomposition (SVD)**:\n",
    "   - Learning about SVD and its applications in solving least squares problems and computing transformations.\n",
    "   - **Source**: [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "\n",
    "3. **Quaternions and Rotations**:\n",
    "   - Understanding how quaternions are used to represent rotations and how to convert them to rotation matrices.\n",
    "   - **Source**: [Quaternions and 3D Rotation](https://www.oreilly.com/library/view/3d-math-primer/9781568817231/)\n",
    "\n",
    "4. **Vector Norms and Linear Algebra**:\n",
    "   - Learning about vector norms and their importance in normalization and other linear algebra operations.\n",
    "   - **Source**: [Vector Norms](https://mathworld.wolfram.com/VectorNorm.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bb4247cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:19.230437Z",
     "iopub.status.busy": "2024-05-21T18:02:19.230120Z",
     "iopub.status.idle": "2024-05-21T18:02:19.247590Z",
     "shell.execute_reply": "2024-05-21T18:02:19.246745Z"
    },
    "papermill": {
     "duration": 0.046481,
     "end_time": "2024-05-21T18:02:19.249520",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.203039",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def affine_matrix_from_points(v0, v1, shear=False, scale=True, usesvd=True):\n",
    "    \n",
    "    v0 = np.array(v0, dtype=np.float64, copy=True)\n",
    "    v1 = np.array(v1, dtype=np.float64, copy=True)\n",
    "\n",
    "    ndims = v0.shape[0]\n",
    "    if ndims < 2 or v0.shape[1] < ndims or v0.shape != v1.shape:\n",
    "        raise ValueError(\"input arrays are of wrong shape or type\")\n",
    "\n",
    "    # move centroids to origin\n",
    "    t0 = -np.mean(v0, axis=1)\n",
    "    M0 = np.identity(ndims + 1)\n",
    "    M0[:ndims, ndims] = t0\n",
    "    v0 += t0.reshape(ndims, 1)\n",
    "    t1 = -np.mean(v1, axis=1)\n",
    "    M1 = np.identity(ndims + 1)\n",
    "    M1[:ndims, ndims] = t1\n",
    "    v1 += t1.reshape(ndims, 1)\n",
    "\n",
    "    if shear:\n",
    "        # Affine transformation\n",
    "        A = np.concatenate((v0, v1), axis=0)\n",
    "        u, s, vh = np.linalg.svd(A.T)\n",
    "        vh = vh[:ndims].T\n",
    "        B = vh[:ndims]\n",
    "        C = vh[ndims: 2 * ndims]\n",
    "        t = np.dot(C, np.linalg.pinv(B))\n",
    "        t = np.concatenate((t, np.zeros((ndims, 1))), axis=1)\n",
    "        M = np.vstack((t, ((0.0,) * ndims) + (1.0,)))\n",
    "    elif usesvd or ndims != 3:\n",
    "        # Rigid transformation via SVD of covariance matrix\n",
    "        u, s, vh = np.linalg.svd(np.dot(v1, v0.T))\n",
    "        # rotation matrix from SVD orthonormal bases\n",
    "        R = np.dot(u, vh)\n",
    "        if np.linalg.det(R) < 0.0:\n",
    "            # R does not constitute right handed system\n",
    "            R -= np.outer(u[:, ndims - 1], vh[ndims - 1, :] * 2.0)\n",
    "            s[-1] *= -1.0\n",
    "        # homogeneous transformation matrix\n",
    "        M = np.identity(ndims + 1)\n",
    "        M[:ndims, :ndims] = R\n",
    "    else:\n",
    "        # Rigid transformation matrix via quaternion\n",
    "        # compute symmetric matrix N\n",
    "        xx, yy, zz = np.sum(v0 * v1, axis=1)\n",
    "        xy, yz, zx = np.sum(v0 * np.roll(v1, -1, axis=0), axis=1)\n",
    "        xz, yx, zy = np.sum(v0 * np.roll(v1, -2, axis=0), axis=1)\n",
    "        N = [\n",
    "            [xx + yy + zz, 0.0, 0.0, 0.0],\n",
    "            [yz - zy, xx - yy - zz, 0.0, 0.0],\n",
    "            [zx - xz, xy + yx, yy - xx - zz, 0.0],\n",
    "            [xy - yx, zx + xz, yz + zy, zz - xx - yy],\n",
    "        ]\n",
    "        # quaternion: eigenvector corresponding to most positive eigenvalue\n",
    "        w, V = np.linalg.eigh(N)\n",
    "        q = V[:, np.argmax(w)]\n",
    "        # print (vector_norm(q), np.linalg.norm(q))\n",
    "        q /= vector_norm(q)  # unit quaternion\n",
    "        # homogeneous transformation matrix\n",
    "        M = quaternion_matrix(q)\n",
    "\n",
    "    if scale and not shear:\n",
    "        # Affine transformation; scale is ratio of RMS deviations from centroid\n",
    "        v0 *= v0\n",
    "        v1 *= v1\n",
    "        M[:ndims, :ndims] *= math.sqrt(np.sum(v1) / np.sum(v0))\n",
    "\n",
    "    # move centroids back\n",
    "    M = np.dot(np.linalg.inv(M1), np.dot(M, M0))\n",
    "    M /= M[ndims, ndims]\n",
    "\n",
    "    # print(\"transformation matrix Python Script: \", M)\n",
    "\n",
    "    return M\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c8ab8f",
   "metadata": {
    "papermill": {
     "duration": 0.025826,
     "end_time": "2024-05-21T18:02:19.301270",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.275444",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5f3c487",
   "metadata": {
    "papermill": {
     "duration": 0.025646,
     "end_time": "2024-05-21T18:02:19.352658",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.327012",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# IMC 3D Error Metric: Register by Horn Method üöÄ\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "This function, `register_by_Horn`, performs registration of estimated camera coordinates (`ev_coord`) to ground truth camera coordinates (`gt_coord`) using an iterative process involving RANSAC and affine transformation. The goal is to find the best transformation matrix that minimizes the error between the estimated and ground truth coordinates.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `register_by_Horn` takes the following parameters:\n",
    "     - `ev_coord`: Estimated camera coordinates.\n",
    "     - `gt_coord`: Ground truth camera coordinates.\n",
    "     - `ransac_threshold`: Thresholds for RANSAC inlier detection.\n",
    "     - `inl_cf`: Inlier coefficient for refining the transformation.\n",
    "     - `strict_cf`: Strict coefficient for determining strict inliers.\n",
    "\n",
    "2. **Remove Invalid Cameras**:\n",
    "   - Cameras with non-finite coordinates are removed from consideration. The indices of valid cameras are stored.\n",
    "\n",
    "3. **Initialization**:\n",
    "   - Several variables are initialized to keep track of the best transformation matrix, errors, and inliers.\n",
    "   - `max_no_inl`: Maximum number of inliers for each RANSAC threshold.\n",
    "   - `best_inl_err`: Best inlier error for each RANSAC threshold.\n",
    "   - `best_transf_matrix`: Best transformation matrix for each RANSAC threshold.\n",
    "   - `best_err`: Best error for each camera and RANSAC threshold.\n",
    "   - `strict_inl`: Boolean array indicating strict inliers.\n",
    "   - `triplets_used`: Indices of the camera triplets used for the best transformation matrix.\n",
    "\n",
    "4. **Run on Camera Triplets**:\n",
    "   - The function iterates over all possible triplets of camera indices. For each triplet:\n",
    "     - If all three cameras are already strict inliers for the best current model, the triplet is skipped.\n",
    "     - An affine transformation matrix is computed using the `affine_matrix_from_points` function.\n",
    "     - The transformation is applied to the estimated camera coordinates.\n",
    "     - The error between the transformed estimated coordinates and ground truth coordinates is computed.\n",
    "     - Inliers are identified based on the error and RANSAC thresholds.\n",
    "     - If the number of inliers for the current triplet is close to the best model so far, the transformation is refined using all inliers.\n",
    "     - The refined transformation matrix and inliers are used to update the best model if they provide a better fit.\n",
    "\n",
    "5. **Best Model**:\n",
    "   - The best model is stored in a dictionary containing:\n",
    "     - `valid_cams`: Indices of valid cameras.\n",
    "     - `no_inl`: Number of inliers for each RANSAC threshold.\n",
    "     - `err`: Error for each camera and RANSAC threshold.\n",
    "     - `triplets_used`: Indices of the camera triplets used for the best transformation matrix.\n",
    "     - `transf_matrix`: Best transformation matrix for each RANSAC threshold.\n",
    "\n",
    "6. **Return**:\n",
    "   - The function returns the best model found.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example estimated and ground truth coordinates\n",
    "ev_coord = np.random.rand(3, 10)  # 3D coordinates for 10 cameras\n",
    "gt_coord = np.random.rand(3, 10)  # 3D ground truth coordinates for 10 cameras\n",
    "\n",
    "# RANSAC parameters\n",
    "ransac_threshold = np.array([0.01, 0.02, 0.05])\n",
    "inl_cf = 1.2\n",
    "strict_cf = 0.8\n",
    "\n",
    "# Compute the best registration model\n",
    "best_model = register_by_Horn(ev_coord, gt_coord, ransac_threshold, inl_cf, strict_cf)\n",
    "\n",
    "# Print the best transformation matrix for the first threshold\n",
    "print(\"Best Transformation Matrix for the first threshold:\\n\", best_model[\"transf_matrix\"][0])\n",
    "```\n",
    "\n",
    "#### Study Sources\n",
    "\n",
    "1. **RANSAC Algorithm**:\n",
    "   - Understanding RANSAC and its applications in robust fitting and outlier detection.\n",
    "   - **Source**: [RANSAC](https://en.wikipedia.org/wiki/Random_sample_consensus)\n",
    "\n",
    "2. **Affine Transformations**:\n",
    "   - Learning about affine transformations and their applications in computer vision and graphics.\n",
    "   - **Source**: [Affine Transformation](https://en.wikipedia.org/wiki/Affine_transformation)\n",
    "\n",
    "3. **Singular Value Decomposition (SVD)**:\n",
    "   - Understanding SVD and its applications in solving least squares problems and computing transformations.\n",
    "   - **Source**: [Singular Value Decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)\n",
    "\n",
    "4. **Horn's Method**:\n",
    "   - Understanding Horn's method for aligning two sets of points using quaternions and least squares fitting.\n",
    "   - **Source**: [Closed-form solution of absolute orientation using unit quaternions](https://people.eecs.berkeley.edu/~ani/teaching/CS294-6/horn.pdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e7a2f16f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:19.406099Z",
     "iopub.status.busy": "2024-05-21T18:02:19.405508Z",
     "iopub.status.idle": "2024-05-21T18:02:19.703337Z",
     "shell.execute_reply": "2024-05-21T18:02:19.702404Z"
    },
    "papermill": {
     "duration": 0.326819,
     "end_time": "2024-05-21T18:02:19.705481",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.378662",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# This is the IMC 3D error metric code\n",
    "def register_by_Horn(ev_coord, gt_coord, ransac_threshold, inl_cf, strict_cf):\n",
    "    \n",
    "    # remove invalid cameras, the index is returned\n",
    "    idx_cams = np.all(np.isfinite(ev_coord), axis=0)\n",
    "    ev_coord = ev_coord[:, idx_cams]\n",
    "    gt_coord = gt_coord[:, idx_cams]\n",
    "\n",
    "    # initialization\n",
    "    n = ev_coord.shape[1]\n",
    "    r = ransac_threshold.shape[0]\n",
    "    ransac_threshold = np.expand_dims(ransac_threshold, axis=0)\n",
    "    ransac_threshold2 = ransac_threshold**2\n",
    "    ev_coord_1 = np.vstack((ev_coord, np.ones(n)))\n",
    "\n",
    "    max_no_inl = np.zeros((1, r))\n",
    "    best_inl_err = np.full(r, np.Inf)\n",
    "    best_transf_matrix = np.zeros((r, 4, 4))\n",
    "    best_err = np.full((n, r), np.Inf)\n",
    "    strict_inl = np.full((n, r), False)\n",
    "    triplets_used = np.zeros((3, r))\n",
    "\n",
    "    # run on camera triplets\n",
    "    for ii in range(n-2):\n",
    "        for jj in range(ii+1, n-1):\n",
    "            for kk in range(jj+1, n):\n",
    "                i = [ii, jj, kk]\n",
    "                triplets_used_now = np.full((n), False)\n",
    "                triplets_used_now[i] = True\n",
    "                # if both ii, jj, kk are strict inliers for the best current model just skip\n",
    "                if np.all(strict_inl[i]):\n",
    "                    continue\n",
    "                # get transformation T by Horn on the triplet camera center correspondences\n",
    "                transf_matrix = affine_matrix_from_points(ev_coord[:, i], gt_coord[:, i], usesvd=False)\n",
    "                # apply transformation T to test camera centres\n",
    "                rotranslated = np.matmul(transf_matrix[:3], ev_coord_1)\n",
    "                # compute error and inliers\n",
    "                err = np.sum((rotranslated - gt_coord)**2, axis=0)\n",
    "                inl = np.expand_dims(err, axis=1) < ransac_threshold2\n",
    "                no_inl = np.sum(inl, axis=0)\n",
    "                # if the number of inliers is close to that of the best model so far, go for refinement\n",
    "                to_ref = np.squeeze(((no_inl > 2) & (no_inl > max_no_inl * inl_cf)), axis=0)\n",
    "                for q in np.argwhere(to_ref):                        \n",
    "                    qq = q[0]\n",
    "                    if np.any(np.all((np.expand_dims(inl[:, qq], axis=1) == inl[:, :qq]), axis=0)):\n",
    "                        # already done for this set of inliers\n",
    "                        continue\n",
    "                    # get transformation T by Horn on the inlier camera center correspondences\n",
    "                    transf_matrix = affine_matrix_from_points(ev_coord[:, inl[:, qq]], gt_coord[:, inl[:, qq]])\n",
    "                    # apply transformation T to test camera centres\n",
    "                    rotranslated = np.matmul(transf_matrix[:3], ev_coord_1)\n",
    "                    # compute error and inliers\n",
    "                    err_ref = np.sum((rotranslated - gt_coord)**2, axis=0)\n",
    "                    err_ref_sum = np.sum(err_ref, axis=0)\n",
    "                    err_ref = np.expand_dims(err_ref, axis=1)\n",
    "                    inl_ref = err_ref < ransac_threshold2\n",
    "                    no_inl_ref = np.sum(inl_ref, axis=0)\n",
    "                    # update the model if better for each threshold\n",
    "                    to_update = np.squeeze((no_inl_ref > max_no_inl) | ((no_inl_ref == max_no_inl) & (err_ref_sum < best_inl_err)), axis=0)\n",
    "                    if np.any(to_update):\n",
    "                        triplets_used[0, to_update] = ii\n",
    "                        triplets_used[1, to_update] = jj\n",
    "                        triplets_used[2, to_update] = kk\n",
    "                        max_no_inl[:, to_update] = no_inl_ref[to_update]\n",
    "                        best_err[:, to_update] = np.sqrt(err_ref)\n",
    "                        best_inl_err[to_update] = err_ref_sum\n",
    "                        strict_inl[:, to_update] = (best_err[:, to_update] < strict_cf * ransac_threshold[:, to_update])\n",
    "                        best_transf_matrix[to_update] = transf_matrix\n",
    "\n",
    "\n",
    "    best_model = {\n",
    "        \"valid_cams\": idx_cams,        \n",
    "        \"no_inl\": max_no_inl,\n",
    "        \"err\": best_err,\n",
    "        \"triplets_used\": triplets_used,\n",
    "        \"transf_matrix\": best_transf_matrix}\n",
    "    return best_model\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73ea4d6",
   "metadata": {
    "papermill": {
     "duration": 0.026118,
     "end_time": "2024-05-21T18:02:19.758368",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.732250",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b0a633",
   "metadata": {
    "papermill": {
     "duration": 0.026061,
     "end_time": "2024-05-21T18:02:19.810866",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.784805",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Mean Average Accuracy (mAA) on Cameras ‚ú®\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The function `mAA_on_cameras` calculates the mean average accuracy (mAA) metric for camera registration errors over a range of thresholds. This metric evaluates the accuracy of camera positions after applying a registration algorithm, using specified error thresholds.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `mAA_on_cameras` takes the following parameters:\n",
    "     - `err`: A 2D array of shape `(n, t)` containing errors for `n` cameras and `t` thresholds.\n",
    "     - `thresholds`: A list or array of error thresholds.\n",
    "     - `n`: The number of cameras.\n",
    "     - `skip_top_thresholds`: The number of top thresholds to skip in the calculation.\n",
    "     - `to_dec`: A constant for adjusting the number of cameras to consider (default is 3).\n",
    "\n",
    "2. **Compute Auxiliary Matrix**:\n",
    "   - The function creates a boolean matrix `aux`, which indicates whether each error value in `err` is below the corresponding threshold from `thresholds`, starting from `skip_top_thresholds`.\n",
    "\n",
    "3. **Calculate mAA**:\n",
    "   - The function calculates the mAA by summing the number of thresholds each camera's error is below, adjusted by `to_dec`.\n",
    "   - The sum is divided by the total number of considered thresholds (`len(thresholds[skip_top_thresholds:])`) and the adjusted number of cameras `(n - to_dec)`.\n",
    "\n",
    "4. **Return**:\n",
    "   - The function returns the mAA value, representing the average accuracy of the camera positions.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Example errors and thresholds\n",
    "errors = np.random.rand(10, 5)  # Errors for 10 cameras and 5 thresholds\n",
    "thresholds = [0.01, 0.02, 0.05, 0.1, 0.2]\n",
    "n = 10\n",
    "skip_top_thresholds = 1\n",
    "to_dec = 3\n",
    "\n",
    "# Compute mAA\n",
    "maa_value = mAA_on_cameras(errors, thresholds, n, skip_top_thresholds, to_dec)\n",
    "print(\"Mean Average Accuracy:\", maa_value)\n",
    "```\n",
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Thresholds**:\n",
    "   - The error thresholds are used to determine if a camera's error is acceptable. Lower thresholds mean stricter accuracy requirements.\n",
    "\n",
    "2. **Auxiliary Matrix**:\n",
    "   - This boolean matrix `aux` helps identify which cameras have errors below the specified thresholds. It's created by comparing each error in `err` to the corresponding threshold in `thresholds`.\n",
    "\n",
    "3. **Mean Average Accuracy (mAA)**:\n",
    "   - mAA is a performance metric that averages the number of times errors fall below given thresholds, adjusted by `to_dec`. It provides an overall measure of registration accuracy.\n",
    "\n",
    "### Practical Applications\n",
    "\n",
    "- **3D Reconstruction**: Evaluating the accuracy of reconstructed camera positions.\n",
    "- **SLAM (Simultaneous Localization and Mapping)**: Assessing the precision of camera localization in SLAM algorithms.\n",
    "- **Augmented Reality**: Ensuring accurate camera placement in augmented reality environments.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7abde718",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:19.866165Z",
     "iopub.status.busy": "2024-05-21T18:02:19.865469Z",
     "iopub.status.idle": "2024-05-21T18:02:19.871208Z",
     "shell.execute_reply": "2024-05-21T18:02:19.870328Z"
    },
    "papermill": {
     "duration": 0.035512,
     "end_time": "2024-05-21T18:02:19.873215",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.837703",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def mAA_on_cameras(err, thresholds, n, skip_top_thresholds, to_dec=3):\n",
    "    \n",
    "    aux = err[:, skip_top_thresholds:] < np.expand_dims(np.asarray(thresholds[skip_top_thresholds:]), axis=0)\n",
    "    return np.sum(np.maximum(np.sum(aux, axis=0) - to_dec, 0)) / (len(thresholds[skip_top_thresholds:]) * (n - to_dec))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc97149",
   "metadata": {
    "papermill": {
     "duration": 0.026024,
     "end_time": "2024-05-21T18:02:19.927089",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.901065",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b94e42",
   "metadata": {
    "papermill": {
     "duration": 0.026329,
     "end_time": "2024-05-21T18:02:19.979753",
     "exception": false,
     "start_time": "2024-05-21T18:02:19.953424",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Function to Extract Camera Centers from DataFrame üì∑\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The function `get_camera_centers_from_df` extracts the camera centers from a given DataFrame. The DataFrame contains columns with the rotation matrix and translation vector for each image, and the function computes the camera center for each image using these values.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `get_camera_centers_from_df` takes a single parameter `df`, which is a pandas DataFrame containing the following columns:\n",
    "     - `image_path`: The file path of the image.\n",
    "     - `rotation_matrix`: The rotation matrix as a semicolon-separated string.\n",
    "     - `translation_vector`: The translation vector as a semicolon-separated string.\n",
    "\n",
    "2. **Initialize Output Dictionary**:\n",
    "   - An empty dictionary `out` is initialized to store the camera centers.\n",
    "\n",
    "3. **Iterate Over DataFrame Rows**:\n",
    "   - The function iterates over each row in the DataFrame using `df.iterrows()`. For each row:\n",
    "     - Extract the file name, rotation matrix, and translation vector.\n",
    "     - Convert the rotation matrix and translation vector from strings to numpy arrays.\n",
    "     - Compute the camera center using the formula \\( \\text{center} = -R^T \\cdot t \\), where \\( R \\) is the rotation matrix and \\( t \\) is the translation vector.\n",
    "     - Store the camera center in the output dictionary with the image path as the key.\n",
    "\n",
    "4. **Return Output Dictionary**:\n",
    "   - The function returns the dictionary `out` containing the camera centers.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example DataFrame\n",
    "data = {\n",
    "    'image_path': ['img1.jpg', 'img2.jpg'],\n",
    "    'rotation_matrix': ['1;0;0;0;1;0;0;0;1', '0.866;0;-0.5;0;1;0;0.5;0;0.866'],\n",
    "    'translation_vector': ['1;2;3', '4;5;6']\n",
    "}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Get camera centers\n",
    "camera_centers = get_camera_centers_from_df(df)\n",
    "print(camera_centers)\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Rotation Matrix and Translation Vector**:\n",
    "   - The rotation matrix \\( R \\) represents the orientation of the camera.\n",
    "   - The translation vector \\( t \\) represents the position of the camera relative to some reference point.\n",
    "\n",
    "2. **Camera Center Calculation**:\n",
    "   - The camera center in world coordinates is calculated as \\( \\text{center} = -R^T \\cdot t \\), where \\( R^T \\) is the transpose of the rotation matrix.\n",
    "\n",
    "3. **DataFrame Iteration**:\n",
    "   - The function uses `df.iterrows()` to iterate over each row in the DataFrame, which allows for accessing and processing each row individually.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9b4aee87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.034074Z",
     "iopub.status.busy": "2024-05-21T18:02:20.033366Z",
     "iopub.status.idle": "2024-05-21T18:02:20.039671Z",
     "shell.execute_reply": "2024-05-21T18:02:20.038784Z"
    },
    "papermill": {
     "duration": 0.035427,
     "end_time": "2024-05-21T18:02:20.041596",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.006169",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_camera_centers_from_df(df):\n",
    "    out = {}\n",
    "    for row in df.iterrows():\n",
    "        row = row[1]\n",
    "        fname = row['image_path']\n",
    "        R = np.array([float(x) for x in (row['rotation_matrix'].split(';'))]).reshape(3,3)\n",
    "        t = np.array([float(x) for x in (row['translation_vector'].split(';'))]).reshape(3)\n",
    "        center = -R.T @ t\n",
    "        out[fname] = center\n",
    "    return out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0e9935",
   "metadata": {
    "papermill": {
     "duration": 0.026194,
     "end_time": "2024-05-21T18:02:20.094137",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.067943",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9baf4f66",
   "metadata": {
    "papermill": {
     "duration": 0.025809,
     "end_time": "2024-05-21T18:02:20.145910",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.120101",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Evaluation of Reconstruction with Mean Average Accuracy (mAA) Metric üìà\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The function `evaluate_rec` evaluates the reconstruction accuracy by comparing the user-provided camera centers with the ground truth camera centers using the mean Average Accuracy (mAA) metric. The process involves extracting camera centers, registering them, and computing the mAA metric.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The function `evaluate_rec` takes the following parameters:\n",
    "     - `gt_df`: Ground truth DataFrame containing camera centers.\n",
    "     - `user_df`: User DataFrame containing predicted camera centers.\n",
    "     - `inl_cf`: Inlier coefficient for RANSAC.\n",
    "     - `strict_cf`: Strict inlier coefficient for RANSAC.\n",
    "     - `skip_top_thresholds`: Number of top thresholds to skip in mAA calculation.\n",
    "     - `to_dec`: Number of cameras to be used in mAA calculation.\n",
    "     - `thresholds`: List of thresholds for RANSAC.\n",
    "\n",
    "2. **Get Camera Centers**:\n",
    "   - Extract camera centers from the ground truth and user DataFrames using the `get_camera_centers_from_df` function.\n",
    "\n",
    "3. **Prepare Data for Evaluation**:\n",
    "   - Initialize a list `good_cams` to store images present in both ground truth and user data.\n",
    "   - Initialize matrices `u_cameras` and `g_cameras` to store the camera centers for the images in `good_cams`.\n",
    "\n",
    "4. **Fill Matrices with Camera Centers**:\n",
    "   - Populate the matrices `u_cameras` and `g_cameras` with the camera centers for the corresponding images in `good_cams`.\n",
    "\n",
    "5. **Register Camera Centers**:\n",
    "   - Register the user camera centers to the ground truth camera centers using the `register_by_Horn` function. This function finds the best transformation matrix for each threshold.\n",
    "\n",
    "6. **Compute mAA**:\n",
    "   - Calculate the mean Average Accuracy (mAA) using the `mAA_on_cameras` function, which measures the accuracy of the registered camera centers against the ground truth.\n",
    "\n",
    "7. **Return mAA**:\n",
    "   - Return the computed mAA value.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example ground truth and user DataFrames\n",
    "gt_data = {\n",
    "    'image_path': ['img1.jpg', 'img2.jpg'],\n",
    "    'rotation_matrix': ['1;0;0;0;1;0;0;0;1', '0.866;0;-0.5;0;1;0;0.5;0;0.866'],\n",
    "    'translation_vector': ['1;2;3', '4;5;6']\n",
    "}\n",
    "user_data = {\n",
    "    'image_path': ['img1.jpg', 'img2.jpg'],\n",
    "    'rotation_matrix': ['0.866;0;-0.5;0;1;0;0.5;0;0.866', '1;0;0;0;1;0;0;0;1'],\n",
    "    'translation_vector': ['1;2;3', '4;5;6']\n",
    "}\n",
    "gt_df = pd.DataFrame(gt_data)\n",
    "user_df = pd.DataFrame(user_data)\n",
    "\n",
    "# Evaluate reconstruction\n",
    "mAA = evaluate_rec(gt_df, user_df)\n",
    "print(f'mAA: {mAA * 100:.2f}%')\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Camera Center Extraction**:\n",
    "   - The camera centers are extracted using the rotation matrix and translation vector for each image.\n",
    "\n",
    "2. **RANSAC (Random Sample Consensus)**:\n",
    "   - RANSAC is used to find the best transformation matrix by iteratively selecting random subsets of the data and fitting a model.\n",
    "\n",
    "3. **mAA (mean Average Accuracy)**:\n",
    "   - mAA measures the accuracy of the registered camera centers against the ground truth camera centers over different thresholds.\n",
    "\n",
    "4. **Horn's Method**:\n",
    "   - Horn's method is used for registering the camera centers by finding the optimal transformation matrix.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c9b4bccf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.200417Z",
     "iopub.status.busy": "2024-05-21T18:02:20.199703Z",
     "iopub.status.idle": "2024-05-21T18:02:20.208806Z",
     "shell.execute_reply": "2024-05-21T18:02:20.207992Z"
    },
    "papermill": {
     "duration": 0.038365,
     "end_time": "2024-05-21T18:02:20.210695",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.172330",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_rec(gt_df, user_df, inl_cf = 0.8, strict_cf=0.5, skip_top_thresholds=2, to_dec=3,\n",
    "                 thresholds=[0.005, 0.01, 0.02, 0.03, 0.04, 0.05, 0.1, 0.15, 0.2]):\n",
    "    # get camera centers\n",
    "    ucameras = get_camera_centers_from_df(user_df)\n",
    "    gcameras = get_camera_centers_from_df(gt_df)    \n",
    "\n",
    "    # the denominator for mAA ratio\n",
    "    m = gt_df.shape[0]\n",
    "    \n",
    "    # get the image list to use\n",
    "    good_cams = []\n",
    "    for image_path in gcameras.keys():\n",
    "        if image_path in ucameras.keys():\n",
    "            good_cams.append(image_path)\n",
    "        \n",
    "    # put corresponding camera centers into matrices\n",
    "    n = len(good_cams)\n",
    "    u_cameras = np.zeros((3, n))\n",
    "    g_cameras = np.zeros((3, n))\n",
    "    \n",
    "    ii = 0\n",
    "    for i in good_cams:\n",
    "        u_cameras[:, ii] = ucameras[i]\n",
    "        g_cameras[:, ii] = gcameras[i]\n",
    "        ii += 1\n",
    "        \n",
    "    # Horn camera centers registration, a different best model for each camera threshold\n",
    "    model = register_by_Horn(u_cameras, g_cameras, np.asarray(thresholds), inl_cf, strict_cf)\n",
    "    \n",
    "    # transformation matrix\n",
    "#     print(\"\\nTransformation matrix for maximum threshold\")\n",
    "    T = np.squeeze(model['transf_matrix'][-1])\n",
    "#     print(T)\n",
    "    \n",
    "    # mAA\n",
    "    mAA = mAA_on_cameras(model[\"err\"], thresholds, m, skip_top_thresholds, to_dec)\n",
    "    # print(f'mAA = {mAA * 100 : .2f}% considering {m} input cameras - {to_dec}')\n",
    "    return mAA\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1795d344",
   "metadata": {
    "papermill": {
     "duration": 0.026327,
     "end_time": "2024-05-21T18:02:20.262896",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.236569",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ccb049",
   "metadata": {
    "papermill": {
     "duration": 0.025802,
     "end_time": "2024-05-21T18:02:20.314748",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.288946",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Scoring Evaluation Function üîçüìä\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The `score` function computes the overall score of a submission by evaluating the reconstruction performance across different scenes or datasets. It calculates the mean Average Accuracy (mAA) across all scenes.\n",
    "\n",
    "1. **Function Definition**:\n",
    "   - The `score` function takes two parameters:\n",
    "     - `solution`: A DataFrame containing ground truth camera centers.\n",
    "     - `submission`: A DataFrame containing user-submitted camera centers.\n",
    "\n",
    "2. **Scene-wise Evaluation**:\n",
    "   - Iterate over each scene in the solution DataFrame.\n",
    "   - Sort both the ground truth and user-submitted DataFrames based on the image paths in ascending order.\n",
    "   - Call the `evaluate_rec` function to compute the reconstruction accuracy (`mAA`) for each scene.\n",
    "\n",
    "3. **Compute Overall Score**:\n",
    "   - Calculate the mean mAA across all scenes to get the overall score for the submission.\n",
    "\n",
    "4. **Return Score**:\n",
    "   - Return the computed score as a float value.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example ground truth and user DataFrames\n",
    "solution_data = {\n",
    "    'image_path': ['img1.jpg', 'img2.jpg'],\n",
    "    'dataset': ['scene1', 'scene1'],\n",
    "    'rotation_matrix': ['1;0;0;0;1;0;0;0;1', '0.866;0;-0.5;0;1;0;0.5;0;0.866'],\n",
    "    'translation_vector': ['1;2;3', '4;5;6']\n",
    "}\n",
    "submission_data = {\n",
    "    'image_path': ['img1.jpg', 'img2.jpg'],\n",
    "    'dataset': ['scene1', 'scene1'],\n",
    "    'rotation_matrix': ['0.866;0;-0.5;0;1;0;0.5;0;0.866', '1;0;0;0;1;0;0;0;1'],\n",
    "    'translation_vector': ['1;2;3', '4;5;6']\n",
    "}\n",
    "solution_df = pd.DataFrame(solution_data)\n",
    "submission_df = pd.DataFrame(submission_data)\n",
    "\n",
    "# Compute score\n",
    "overall_score = score(solution_df, submission_df)\n",
    "print(f'Overall Score: {overall_score:.4f}')\n",
    "```\n",
    "\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Scoring Function**:\n",
    "   - The `score` function evaluates the overall performance of a submission by computing the mean mAA across different scenes.\n",
    "\n",
    "2. **Scene-wise Evaluation**:\n",
    "   - It iterates over each scene and computes the mAA metric for that scene individually.\n",
    "\n",
    "3. **Mean Average Accuracy (mAA)**:\n",
    "   - mAA measures the accuracy of the registered camera centers against the ground truth camera centers.\n",
    "\n",
    "4. **Pandas DataFrame Operations**:\n",
    "   - It performs sorting and filtering operations on DataFrames to process ground truth and user-submitted data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4e1f4adb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.368659Z",
     "iopub.status.busy": "2024-05-21T18:02:20.367971Z",
     "iopub.status.idle": "2024-05-21T18:02:20.375492Z",
     "shell.execute_reply": "2024-05-21T18:02:20.374618Z"
    },
    "papermill": {
     "duration": 0.036688,
     "end_time": "2024-05-21T18:02:20.377327",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.340639",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def score(solution: pd.DataFrame, submission: pd.DataFrame) -> float:\n",
    "    \n",
    "    scenes = list(set(solution['dataset'].tolist()))\n",
    "    results_per_dataset = []\n",
    "    for dataset in scenes:\n",
    "        print(f\"\\n*** {dataset} ***\")\n",
    "#         start = time.time()\n",
    "        gt_ds = solution[solution['dataset'] == dataset]\n",
    "        user_ds = submission[submission['dataset'] == dataset]\n",
    "        gt_ds = gt_ds.sort_values(by=['image_path'], ascending = True)\n",
    "        user_ds = user_ds.sort_values(by=['image_path'], ascending = True)\n",
    "        result = evaluate_rec(gt_ds, user_ds, inl_cf=0, strict_cf=-1, skip_top_thresholds=0, to_dec=3,\n",
    "                 thresholds=translation_thresholds_meters_dict[dataset])\n",
    "#         end = time.time()\n",
    "        print(f\"\\nmAA: {round(result,4)}\")\n",
    "#         print(\"Running time: %s\" % (end - start))        \n",
    "        results_per_dataset.append(result)\n",
    "    return float(np.array(results_per_dataset).mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "850b9543",
   "metadata": {
    "papermill": {
     "duration": 0.031581,
     "end_time": "2024-05-21T18:02:20.437271",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.405690",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6f2657",
   "metadata": {
    "papermill": {
     "duration": 0.026062,
     "end_time": "2024-05-21T18:02:20.489518",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.463456",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Function to Generate Image Pairs for Matching üñºÔ∏èüîç\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The `get_pairs` function generates pairs of images from a list of images for matching. It considers two modes of operation: exhaustive and non-exhaustive.\n",
    "\n",
    "1. **Exhaustive Mode**:\n",
    "   - If the `EXHAUSTIVE` flag is set to `True`, the function generates all possible combinations of image pairs using `itertools.combinations`.\n",
    "\n",
    "2. **Non-exhaustive Mode**:\n",
    "   - If the flag is set to `False`, the function extracts features from each image using a pre-trained DINOv2 model.\n",
    "   - It computes embeddings for each image and calculates pairwise distances between embeddings.\n",
    "   - Image pairs with distances below a threshold (`DISTANCES_THRESHOLD`) are considered potential matches.\n",
    "   - To ensure a minimum number of pairs (`MIN_PAIRS`) for each image, additional pairs are selected based on distance rankings.\n",
    "   - Pairs with distances above a tolerance level (`TOLERANCE`) are filtered out.\n",
    "   - The function returns a list of unique pairs of indices representing matching image pairs.\n",
    "\n",
    "3. **Input**:\n",
    "   - `images_list`: A list of image paths.\n",
    "   - `device`: The device (CPU or GPU) to use for processing.\n",
    "\n",
    "4. **Output**:\n",
    "   - A list of tuples representing pairs of indices, where each tuple contains two indices indicating the positions of matching images in the input list.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "# Example usage with a list of image paths\n",
    "images_list = ['image1.jpg', 'image2.jpg', 'image3.jpg']\n",
    "pairs = get_pairs(images_list)\n",
    "print(pairs)\n",
    "```\n",
    "\n",
    "### Key Concepts\n",
    "\n",
    "1. **Feature Extraction with DINOv2**:\n",
    "   - The function utilizes a pre-trained DINOv2 model to extract embeddings from images.\n",
    "   - These embeddings represent the features of each image.\n",
    "\n",
    "2. **Pairwise Distance Calculation**:\n",
    "   - It computes pairwise distances between the embeddings of all images.\n",
    "   - Images with distances below a certain threshold are considered potential matches.\n",
    "\n",
    "3. **Filtering and Ranking**:\n",
    "   - The function filters out pairs with distances above a tolerance level.\n",
    "   - It ensures a minimum number of pairs for each image by selecting additional pairs based on distance rankings.\n",
    "\n",
    "4. **Optimization**:\n",
    "   - The function optimizes the process to handle large datasets efficiently by computing distances only once.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "7fcf2729",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.543496Z",
     "iopub.status.busy": "2024-05-21T18:02:20.542681Z",
     "iopub.status.idle": "2024-05-21T18:02:20.553744Z",
     "shell.execute_reply": "2024-05-21T18:02:20.552909Z"
    },
    "papermill": {
     "duration": 0.040139,
     "end_time": "2024-05-21T18:02:20.555648",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.515509",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_pairs(images_list,device=DEVICE):\n",
    "    if EXHAUSTIVE:\n",
    "        return list(combinations(range(len(images_list)), 2)) \n",
    "    \n",
    "    processor = AutoImageProcessor.from_pretrained('/kaggle/input/dinov2/pytorch/base/1/')\n",
    "    model = AutoModel.from_pretrained('/kaggle/input/dinov2/pytorch/base/1/').eval().to(DEVICE)\n",
    "    embeddings = []\n",
    "    \n",
    "    for img_path in images_list:\n",
    "        image = K.io.load_image(img_path, K.io.ImageLoadType.RGB32, device=DEVICE)[None, ...]\n",
    "        with torch.inference_mode():\n",
    "            inputs = processor(images=image, return_tensors=\"pt\", do_rescale=False ,do_resize=True, \n",
    "                               do_center_crop=True, size=224).to(DEVICE)\n",
    "            outputs = model(**inputs)\n",
    "            embedding = F.normalize(outputs.last_hidden_state.max(dim=1)[0])\n",
    "        embeddings.append(embedding)\n",
    "        \n",
    "    embeddings = torch.cat(embeddings, dim=0)\n",
    "    distances = torch.cdist(embeddings,embeddings).cpu()\n",
    "    distances_ = (distances <= DISTANCES_THRESHOLD).numpy()\n",
    "    np.fill_diagonal(distances_,False)\n",
    "    z = distances_.sum(axis=1)\n",
    "    idxs0 = np.where(z == 0)[0]\n",
    "    for idx0 in idxs0:\n",
    "        t = np.argsort(distances[idx0])[1:MIN_PAIRS]\n",
    "        distances_[idx0,t] = True\n",
    "        \n",
    "    s = np.where(distances >= TOLERANCE)\n",
    "    distances_[s] = False\n",
    "    \n",
    "    idxs = []\n",
    "    for i in range(len(images_list)):\n",
    "        for j in range(len(images_list)):\n",
    "            if distances_[i][j]:\n",
    "                idxs += [(i,j)] if i<j else [(j,i)]\n",
    "    \n",
    "    idxs = list(set(idxs))\n",
    "    return idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa0b6b0e",
   "metadata": {
    "papermill": {
     "duration": 0.026003,
     "end_time": "2024-05-21T18:02:20.607872",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.581869",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14a31477",
   "metadata": {
    "papermill": {
     "duration": 0.026034,
     "end_time": "2024-05-21T18:02:20.660014",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.633980",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Matching Keypoints between Images üì∏üîë\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The `keypoints_matches` function matches keypoints between pairs of images from a list of images.\n",
    "\n",
    "1. **Feature Extraction and Matching**:\n",
    "   - It utilizes ALIKED for keypoint detection and LightGlueMatcher for matching keypoints between images.\n",
    "   - The function first extracts keypoints and descriptors for each image using ALIKED.\n",
    "   - It then matches descriptors between pairs of images using LightGlueMatcher.\n",
    "   \n",
    "2. **Creating and Storing Keypoints and Descriptors**:\n",
    "   - Keypoints and descriptors are saved in separate HDF5 files (`keypoints.h5` and `descriptors.h5`).\n",
    "   - Each image's keypoints and descriptors are stored under their respective image names in the HDF5 files.\n",
    "\n",
    "3. **Storing Matches**:\n",
    "   - Matches between keypoints of image pairs are stored in another HDF5 file (`matches.h5`).\n",
    "   - For each pair of images, if the number of matches exceeds a minimum threshold (`MIN_MATCHES`), the indices of matched keypoints are stored.\n",
    "\n",
    "#### Input Parameters\n",
    "\n",
    "- `images_list`: A list of image paths.\n",
    "- `pairs`: A list of tuples representing pairs of indices of matching images.\n",
    "\n",
    "#### Output\n",
    "\n",
    "- Keypoints, descriptors, and matches are stored in HDF5 files (`keypoints.h5`, `descriptors.h5`, and `matches.h5`, respectively).\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "# Example usage with a list of image paths and pairs of matching images\n",
    "images_list = ['image1.jpg', 'image2.jpg', 'image3.jpg']\n",
    "pairs = [(0, 1), (1, 2)]  # Example pairs of matching images\n",
    "keypoints_matches(images_list, pairs)\n",
    "```\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **Keypoint Detection and Description**:\n",
    "   - ALIKED is used for keypoint detection and description, providing robust feature extraction.\n",
    "   \n",
    "2. **Keypoint Matching**:\n",
    "   - LightGlueMatcher performs matching between keypoints of different images based on their descriptors.\n",
    "   - Matches are stored as indices of keypoints in the HDF5 file.\n",
    "\n",
    "3. **Efficient Storage**:\n",
    "   - HDF5 format is utilized for efficient storage of keypoints, descriptors, and matches.\n",
    "\n",
    "4. **Data Persistence**:\n",
    "   - The function ensures data persistence by storing keypoints, descriptors, and matches in HDF5 files, facilitating later retrieval and analysis.\n",
    "\n",
    "### References\n",
    "- [ALIKED: An Automatic LInkage of Keypoint Detection](https://arxiv.org/abs/2110.00665)\n",
    "- [LightGlueMatcher: Efficient Keypoint Matching](https://github.com/rpautrat/LightGlueMatcher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "01bebe87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.714566Z",
     "iopub.status.busy": "2024-05-21T18:02:20.713731Z",
     "iopub.status.idle": "2024-05-21T18:02:20.726062Z",
     "shell.execute_reply": "2024-05-21T18:02:20.725192Z"
    },
    "papermill": {
     "duration": 0.041645,
     "end_time": "2024-05-21T18:02:20.727904",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.686259",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def keypoints_matches(images_list,pairs):\n",
    "    extractor = ALIKED(max_num_keypoints=MAX_NUM_KEYPOINTS,detection_threshold=DETECTION_THRESHOLD,resize=RESIZE_TO).eval().to(DEVICE)\n",
    "    matcher = KF.LightGlueMatcher(\"aliked\", {'width_confidence':-1, 'depth_confidence':-1, 'mp':True if 'cuda' in str(DEVICE) else False}).eval().to(DEVICE)\n",
    "    rotation = create_model(\"swsl_resnext50_32x4d\").eval().to(DEVICE)\n",
    "    \n",
    "    with h5py.File(\"keypoints.h5\", mode=\"w\") as f_kp, h5py.File(\"descriptors.h5\", mode=\"w\") as f_desc:  \n",
    "        for image_path in images_list:\n",
    "            with torch.inference_mode():\n",
    "                image = load_image(image_path).to(DEVICE)\n",
    "                feats = extractor.extract(image)\n",
    "                f_kp[image_path.name] = feats[\"keypoints\"].squeeze().cpu().numpy()\n",
    "                f_desc[image_path.name] = feats[\"descriptors\"].squeeze().detach().cpu().numpy()\n",
    "                \n",
    "    with h5py.File(\"keypoints.h5\", mode=\"r\") as f_kp, h5py.File(\"descriptors.h5\", mode=\"r\") as f_desc, \\\n",
    "         h5py.File(\"matches.h5\", mode=\"w\") as f_matches:  \n",
    "        for pair in pairs:\n",
    "            key1, key2 = images_list[pair[0]].name, images_list[pair[1]].name\n",
    "            kp1 = torch.from_numpy(f_kp[key1][...]).to(DEVICE)\n",
    "            kp2 = torch.from_numpy(f_kp[key2][...]).to(DEVICE)\n",
    "            desc1 = torch.from_numpy(f_desc[key1][...]).to(DEVICE)\n",
    "            desc2 = torch.from_numpy(f_desc[key2][...]).to(DEVICE)\n",
    "            with torch.inference_mode():\n",
    "                _, idxs = matcher(desc1, desc2, KF.laf_from_center_scale_ori(kp1[None]), KF.laf_from_center_scale_ori(kp2[None]))\n",
    "            if len(idxs): group = f_matches.require_group(key1)\n",
    "            if len(idxs) >= MIN_MATCHES: group.create_dataset(key2, data=idxs.detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f95539",
   "metadata": {
    "papermill": {
     "duration": 0.026584,
     "end_time": "2024-05-21T18:02:20.780709",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.754125",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262fb1f4",
   "metadata": {
    "papermill": {
     "duration": 0.026685,
     "end_time": "2024-05-21T18:02:20.833702",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.807017",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# RANSAC and Sparse Reconstruction Algorithm üèóÔ∏èüîç\n",
    "\n",
    "#### Explanation of the Code\n",
    "\n",
    "The `ransac_and_sparse_reconstruction` function performs RANSAC-based matching and sparse reconstruction using COLMAP.\n",
    "\n",
    "1. **Database Creation**:\n",
    "   - A new COLMAP database is created with a unique name based on the current timestamp.\n",
    "   - Tables for keypoints, matches, and other necessary entities are initialized in the database.\n",
    "\n",
    "2. **Adding Keypoints and Matches**:\n",
    "   - Keypoints are extracted from images in the specified path using a pinhole camera model.\n",
    "   - Keypoints are added to the COLMAP database along with their descriptors.\n",
    "   - Matches between keypoints are computed and added to the database.\n",
    "\n",
    "3. **Matching and Reconstruction**:\n",
    "   - Exhaustive matching is performed using SIFT features.\n",
    "   - Incremental mapping is executed to generate a sparse reconstruction of the scene.\n",
    "   - The resulting reconstructions are returned.\n",
    "\n",
    "4. **Output**:\n",
    "   - The function returns the sparse reconstructions generated by COLMAP.\n",
    "\n",
    "#### Input Parameter\n",
    "\n",
    "- `images_path`: Path to the directory containing images to be reconstructed.\n",
    "\n",
    "#### Output\n",
    "\n",
    "- Sparse reconstructions of the scene obtained from COLMAP.\n",
    "\n",
    "#### Example Usage\n",
    "\n",
    "```python\n",
    "# Example usage with the path to the directory containing images\n",
    "images_path = '/path/to/images/directory'\n",
    "sparse_reconstructions = ransac_and_sparse_reconstruction(images_path)\n",
    "```\n",
    "\n",
    "#### Key Concepts\n",
    "\n",
    "1. **RANSAC (Random Sample Consensus)**:\n",
    "   - RANSAC is utilized for robust estimation of geometric transformations from noisy data.\n",
    "   - It helps in identifying reliable matches between keypoints despite outliers.\n",
    "\n",
    "2. **Sparse Reconstruction**:\n",
    "   - COLMAP performs sparse reconstruction by estimating the 3D structure of the scene from images.\n",
    "   - It generates a set of sparse 3D points and camera poses, representing the scene geometry.\n",
    "\n",
    "3. **Incremental Mapping**:\n",
    "   - COLMAP's incremental mapping algorithm incrementally adds images and performs bundle adjustment to refine the scene structure.\n",
    "\n",
    "4. **Efficient Database Management**:\n",
    "   - COLMAP database efficiently manages keypoints, matches, and other data structures required for reconstruction.\n",
    "\n",
    "### References\n",
    "- [COLMAP: Efficient Reconstruction of 3D Scenes from Images](https://colmap.github.io/)\n",
    "- [RANSAC: Random Sample Consensus](https://www.cs.cmu.edu/~16385/s17/Slides/11.2_RANSAC.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "79a70f95",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:20.889053Z",
     "iopub.status.busy": "2024-05-21T18:02:20.887902Z",
     "iopub.status.idle": "2024-05-21T18:02:20.895546Z",
     "shell.execute_reply": "2024-05-21T18:02:20.894774Z"
    },
    "papermill": {
     "duration": 0.037373,
     "end_time": "2024-05-21T18:02:20.897388",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.860015",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ransac_and_sparse_reconstruction(images_path):\n",
    "    now = datetime.datetime.now()\n",
    "    time_str = now.strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "    db_name = f'colmap_{time_str}.db'\n",
    "    db = COLMAPDatabase.connect(db_name)\n",
    "    db.create_tables()\n",
    "    fname_to_id = add_keypoints(db, '/kaggle/working/', images_path, '', 'simple-pinhole', False)\n",
    "    add_matches(db, '/kaggle/working/',fname_to_id)\n",
    "    db.commit()\n",
    "    \n",
    "    pycolmap.match_exhaustive(db_name, sift_options={'num_threads':1})\n",
    "    maps = pycolmap.incremental_mapping(\n",
    "        database_path=db_name, \n",
    "        image_path=images_path,\n",
    "        output_path='/kaggle/working/', \n",
    "        options=pycolmap.IncrementalPipelineOptions({'min_model_size':MIN_MODEL_SIZE, 'max_num_models':MAX_NUM_MODELS, 'num_threads':1})\n",
    "    )\n",
    "    return maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bba347a",
   "metadata": {
    "papermill": {
     "duration": 0.026747,
     "end_time": "2024-05-21T18:02:20.950699",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.923952",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a742084c",
   "metadata": {
    "papermill": {
     "duration": 0.026508,
     "end_time": "2024-05-21T18:02:21.003969",
     "exception": false,
     "start_time": "2024-05-21T18:02:20.977461",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Parameters Configuration üõ†Ô∏è\n",
    "\n",
    "#### Similar Pairs Detection Parameters:\n",
    "- **Exhaustive**: Determines whether exhaustive pair matching is enabled.\n",
    "- **Minimum Pairs**: Minimum number of pairs required for exhaustive matching.\n",
    "- **Distances Threshold**: Threshold for pairwise distance comparison.\n",
    "- **Tolerance**: Tolerance value for distance comparison.\n",
    "\n",
    "#### Keypoints Extractor and Matcher Parameters:\n",
    "- **Max Number of Keypoints**: Maximum number of keypoints to extract from each image.\n",
    "- **Resize Dimension**: Dimensions to which images are resized before keypoints extraction.\n",
    "- **Detection Threshold**: Threshold for keypoints detection.\n",
    "- **Minimum Matches**: Minimum number of matches required for keypoints matching.\n",
    "\n",
    "#### RANSAC and Sparse Reconstruction Parameters:\n",
    "- **Minimum Model Size**: Minimum number of images required for a valid reconstruction model.\n",
    "- **Maximum Number of Models**: Maximum number of reconstruction models to generate.\n",
    "\n",
    "#### Cross-Validation Parameters:\n",
    "- **Number of Samples**: Number of samples used for cross-validation.\n",
    "\n",
    "#### Submission Control:\n",
    "- **Submission**: Determines whether the system should generate a submission.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6f12a5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:21.059560Z",
     "iopub.status.busy": "2024-05-21T18:02:21.058804Z",
     "iopub.status.idle": "2024-05-21T18:02:21.064077Z",
     "shell.execute_reply": "2024-05-21T18:02:21.063197Z"
    },
    "papermill": {
     "duration": 0.035398,
     "end_time": "2024-05-21T18:02:21.066029",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.030631",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SIMILLIAR PAIRS\n",
    "EXHAUSTIVE = True\n",
    "MIN_PAIRS = 50\n",
    "DISTANCES_THRESHOLD = 0.3\n",
    "TOLERANCE = 500\n",
    "\n",
    "# KEYPOINTS EXTRACTOR AND MATCHER\n",
    "MAX_NUM_KEYPOINTS = 4096\n",
    "RESIZE_TO = 1280\n",
    "DETECTION_THRESHOLD = 0.005\n",
    "MIN_MATCHES = 100\n",
    "\n",
    "# RANSAC AND SPARSE RECONSTRUCTION\n",
    "MIN_MODEL_SIZE = 4\n",
    "MAX_NUM_MODELS = 3\n",
    "\n",
    "# CROSS VALIDATION\n",
    "N_SAMPLES = 50\n",
    "\n",
    "SUBMISSION = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d207b7db",
   "metadata": {
    "papermill": {
     "duration": 0.026071,
     "end_time": "2024-05-21T18:02:21.119780",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.093709",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b543dd7",
   "metadata": {
    "papermill": {
     "duration": 0.026612,
     "end_time": "2024-05-21T18:02:21.172769",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.146157",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "1. Defining a function `image_path(row)` to create the image paths for the train dataset.\n",
    "2. Reading the train dataset from a CSV file and applying the `image_path` function to generate the image paths.\n",
    "3. Sampling a specified number of image paths from each group in the train dataset.\n",
    "4. Creating a ground truth dataframe (`gt_df`) containing the sampled image paths.\n",
    "5. Creating a prediction dataframe (`pred_df`) from the ground truth dataframe.\n",
    "6. Saving the prediction dataframe to a CSV file named `pred_df.csv`.\n",
    "7. Running the evaluation pipeline (`run`) with the prediction dataframe.\n",
    "8. Reading the generated submission CSV file (`submission.csv`).\n",
    "9. Calculating the mean Average Accuracy (mAA) score between the ground truth and prediction dataframes.\n",
    "\n",
    "Finally, the total mAA score is printed.\n",
    "\n",
    "### Example Usage\n",
    "```python\n",
    "if not SUBMISSION:\n",
    "    # Execute the operations\n",
    "```\n",
    "\n",
    "Ensure that the `SUBMISSION` variable is appropriately set before running this code block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab4e088e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:21.227825Z",
     "iopub.status.busy": "2024-05-21T18:02:21.227271Z",
     "iopub.status.idle": "2024-05-21T18:02:21.236354Z",
     "shell.execute_reply": "2024-05-21T18:02:21.235484Z"
    },
    "papermill": {
     "duration": 0.039015,
     "end_time": "2024-05-21T18:02:21.238240",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.199225",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if not SUBMISSION:\n",
    "    def image_path(row):\n",
    "        row['image_path'] = 'train/' + row['dataset'] + '/images/' + row['image_name']\n",
    "        return row\n",
    "\n",
    "    train_df = pd.read_csv(f'{IMC_PATH}/train/train_labels.csv')\n",
    "    train_df = train_df.apply(image_path,axis=1).drop_duplicates(subset=['image_path'])\n",
    "    G = train_df.groupby(['dataset','scene'])['image_path']\n",
    "    image_paths = []\n",
    "    \n",
    "    for g in G:\n",
    "        n = N_SAMPLES\n",
    "        n = n if n < len(g[1]) else len(g[1])\n",
    "        g = g[0],g[1].sample(n,random_state=42).reset_index(drop=True)\n",
    "        for image_path in g[1]:\n",
    "            image_paths.append(image_path)\n",
    "        \n",
    "    gt_df = train_df[train_df.image_path.isin(image_paths)].reset_index(drop=True)\n",
    "    pred_df = gt_df[['image_path','dataset','scene','rotation_matrix','translation_vector']]\n",
    "    pred_df.to_csv('pred_df.csv',index=False)\n",
    "    run('pred_df.csv', get_pairs, keypoints_matches, ransac_and_sparse_reconstruction, submit=False)\n",
    "    pred_df = pd.read_csv('submission.csv')\n",
    "    mAA = round(score(gt_df, pred_df),4)\n",
    "    print('*** Total mean Average Accuracy ***')\n",
    "    print(f\"mAA: {mAA}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611c4194",
   "metadata": {
    "papermill": {
     "duration": 0.027126,
     "end_time": "2024-05-21T18:02:21.292484",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.265358",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6ca894",
   "metadata": {
    "papermill": {
     "duration": 0.027057,
     "end_time": "2024-05-21T18:02:21.346566",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.319509",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Explaination**\n",
    "\n",
    "1. It specifies the `data_path` variable to the location of the sample submission CSV file.\n",
    "2. It calls the `run` function with the specified parameters: `data_path`, `get_pairs`, `keypoints_matches`, and `ransac_and_sparse_reconstruction`.\n",
    "\n",
    "This block essentially runs the entire pipeline for generating submissions using the provided functions and sample submission data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "80d808f6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-21T18:02:21.401941Z",
     "iopub.status.busy": "2024-05-21T18:02:21.401634Z",
     "iopub.status.idle": "2024-05-21T18:09:10.955497Z",
     "shell.execute_reply": "2024-05-21T18:09:10.954666Z"
    },
    "papermill": {
     "duration": 409.583779,
     "end_time": "2024-05-21T18:09:10.957675",
     "exception": false,
     "start_time": "2024-05-21T18:02:21.373896",
     "status": "completed"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "if SUBMISSION:\n",
    "    data_path = IMC_PATH + \"/sample_submission.csv\"\n",
    "    run(data_path, get_pairs, keypoints_matches, ransac_and_sparse_reconstruction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27034d03",
   "metadata": {
    "papermill": {
     "duration": 0.026817,
     "end_time": "2024-05-21T18:09:11.012375",
     "exception": false,
     "start_time": "2024-05-21T18:09:10.985558",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55c1a70e",
   "metadata": {
    "papermill": {
     "duration": 0.027276,
     "end_time": "2024-05-21T18:09:11.067136",
     "exception": false,
     "start_time": "2024-05-21T18:09:11.039860",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## üåü Keep Exploring! üåü\n",
    "\n",
    "Thanks a bunch for diving into this notebook! If you had a blast or learned something new, why not dive into more of my captivating projects and contributions on my profile?\n",
    "\n",
    "üëâ [Let's Explore More!](https://www.kaggle.com/zulqarnainalipk) üëà\n",
    "\n",
    "[GitHub](https://github.com/zulqarnainalipk) |\n",
    "[LinkedIn](https://www.linkedin.com/in/zulqarnainalipk/)\n",
    "\n",
    "## üí¨ Share Your Thoughts! üí°\n",
    "\n",
    "Your feedback is like treasure to us! Your brilliant ideas and insights fuel our ongoing improvement. Got something to say, ask, or suggest? Don't hold back!\n",
    "\n",
    "üì¨ Drop me a line via email: [zulqar445ali@gmail.com](mailto:zulqar445ali@gmail.com)\n",
    "\n",
    "Huge thanks for your time and engagement. Your support is like rocket fuel propelling me to create even more epic content.\n",
    "Keep coding joyfully and wishing you stellar success in your data science adventures! üöÄ\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 8143495,
     "sourceId": 71885,
     "sourceType": "competition"
    },
    {
     "datasetId": 4628051,
     "sourceId": 7884485,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4628331,
     "sourceId": 7884725,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 172469456,
     "sourceType": "kernelVersion"
    },
    {
     "modelInstanceId": 3326,
     "sourceId": 4534,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14317,
     "sourceId": 17191,
     "sourceType": "modelInstanceVersion"
    },
    {
     "modelInstanceId": 14611,
     "sourceId": 17555,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 486.430245,
   "end_time": "2024-05-21T18:09:14.030635",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-05-21T18:01:07.600390",
   "version": "2.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
